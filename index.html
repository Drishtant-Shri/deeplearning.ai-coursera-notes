



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/deeplearning.ai-coursera-notes/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Deeplearning.ai - Coursera Course Notes</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#ff7043">
      
    
    
      <script src="assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="deep-orange" data-md-color-accent="deep-purple">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#deeplearningai-coursera-course-notes" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/deeplearning.ai-coursera-notes/" title="Deeplearning.ai - Coursera Course Notes" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Deeplearning.ai - Coursera Course Notes
              </span>
              <span class="md-header-nav__topic">
                About
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/deeplearning.ai-coursera-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/mathematics-for-machine-learning
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/deeplearning.ai-coursera-notes/" title="Deeplearning.ai - Coursera Course Notes" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Deeplearning.ai - Coursera Course Notes
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/deeplearning.ai-coursera-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/mathematics-for-machine-learning
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
    <a href="." title="About" class="md-nav__link md-nav__link--active">
      About
    </a>
    
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Structuring Machine Learning Projects
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Structuring Machine Learning Projects
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="structuring_machine_learning_projects/week_1.md" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Sequence Models
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Sequence Models
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="sequence_models/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="sequence_models/week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="sequence_models/week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/deeplearning.ai-coursera-notes/edit/master/docs/index.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="deeplearningai-coursera-course-notes">Deeplearning.ai - Coursera Course Notes</h1>
<p>Compiled notes for the deep learning.ai specialization on Coursera.</p>
<p>I intend to continuously add to and improve these notes as I complete the <a href="https://www.deeplearning.ai/">5-course specialization</a>.</p>
<h1 id="course-1-neural-networks-and-deep-learning">Course 1: Neural Networks and Deep Learning</h1>
<h4 id="toc">TOC</h4>
<ol>
<li><a href="#week-1-introduction">Week 1: Introduction</a></li>
<li><a href="#week-2-neural-networks-basics">Week 2: Neural networks basics</a></li>
<li><a href="#week-3-shallow-neural-networks">Week 3: Shallow neural networks</a></li>
<li><a href="#week-4-deep-neural-networks">Week 4: Deep Neural Networks</a></li>
</ol>
<h4 id="resources">Resources</h4>
<ul>
<li><a href="https://d3c33hcgiwev3.cloudfront.net/_106ac679d8102f2bee614cc67e9e5212_deep-learning-notation.pdf?Expires=1514764800&amp;Signature=bnnZZMJUAG2PZPWezLLN6EeKjihlTdaVPo1fqHDdsPmXkLDyjVG-fBtstgOCIcFkKd8OGx845pIKDITTFGm0sMA1eGo4lAIqP7Btffy5VGBRwasKW3WCGGkP-dmq0Vw7Y83ezax4wQCzzYB6iPevY8QniePzg-iq~O5a9hJ4TRk_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A">Notation cheetsheet</a> (I recommend printing this out and sticking it on the wall where you work!)</li>
<li>Check links at end of all programming assignments, these are good resources.</li>
</ul>
<h2 id="week-1-introduction">Week 1: Introduction</h2>
<h3 id="what-is-a-neural-network">What is a neural network?</h3>
<h4 id="supervised-learning-with-neural-networks">Supervised Learning with Neural Networks</h4>
<p>In supervised learning, you have some input \(x\) and some output \(y\) . The goal is to learn a mapping \(x \rightarrow y\) .</p>
<p>Possibly, the single most lucrative (but not the most inspiring) application of deep learning today is online advertising. Using information about the ad combined with information about the user as input, neural networks have gotten very good at predicting whether or not you click on an ad. Because the ability to show you ads that you're more likely to click on has a <em>direct impact on the bottom line of some of the very large online advertising companies</em>.</p>
<p>Here are some more areas in which deep learning has had a huge impact:</p>
<ul>
<li><strong>Computer vision</strong> the recognition and classification of objects in photos and videos.</li>
<li><strong>Speech Recognition</strong> converting speech in audio files into transcribed text.</li>
<li><strong>Machine translation</strong> translating one natural language to another.</li>
<li><strong>Autonomous driving</strong></li>
</ul>
<p><a href="https://postimg.cc/image/r2br1relb/"><img alt="supervised_learning.png" src="https://s19.postimg.cc/wdqnmh0o3/supervised_learning.png" /></a></p>
<p>A lot of the value generation from using neural networks have come from intelligently choosing our \(x\) and \(y\) and learning a mapping.</p>
<p>We tend to use different architectures for different types of data. For example, <strong>convolutional neural networks</strong> (CNNs) are very common for <em>image data</em>, while <strong>recurrent neural networks</strong> (RNNs) are very common for <em>sequence data</em> (such as text). Some data, such as radar data from autonomous vehicles, don't neatly fit into any particularly category and so we typical use a complex/hybrid network architecture.</p>
<h4 id="structured-vs-unstructured-data">Structured vs. Unstructured Data</h4>
<p>You can think of <strong>structured data</strong> as essentially meaning <em>databases of data</em>. It is data that is highly <em>structured</em>, typically with multiple, well-defined attributes for each piece of data. For example, in housing price prediction, you might have a database where the columns tells you the size and the number of bedrooms. Or in predicting whether or not a user will click on an ad, you might have information about the user, such as the age, some information about the ad, and then labels why that you're trying to predict.</p>
<p>In contrast, <strong>unstructured data</strong> refers to things like audio, raw audio,
or images. Here the features might be the pixel values in an image or the individual words in a piece of text. Historically, it has been much harder for computers to make sense of unstructured data compared to structured data. In contrast, the human race has evolved to be very good at understanding audio cues as well as images. <em>People are really good at interpreting unstructured data</em>. And so one of the most exciting things about the rise of neural networks is that, thanks to deep learning, thanks to neural networks, computers are now much better at interpreting unstructured data as compared to just a few years ago. This creates opportunities for many new exciting applications that use speech recognition, image recognition, and natural language processing on text.</p>
<p>Because people have a natural empathy to understanding unstructured data, you might hear about neural network successes on unstructured data more in the media because it's just cool when the neural network recognizes a cat. We all like that, and we all know what that means. But it turns out that a lot of short term economic value that neural networks are creating has also been on structured data, such as much better advertising systems, much better profit recommendations, and just a much better ability to process the giant databases that many companies have to make accurate predictions from them.</p>
<p><a href="https://postimg.cc/image/5ty2jrutb/"><img alt="unstructured_vs_structured_data.png" src="https://s19.postimg.cc/66pgpyd37/unstructured_vs_structured_data.png" /></a></p>
<h3 id="why-is-deep-learning-taking-off">Why is Deep Learning taking off?</h3>
<p><em>If the basic technical details surrounding deep learning have been around for decades, why are they just taking off now?</em></p>
<p>First and foremost, the massive amount of (labeled) data we have been generating for the past couple of decades (in part because of the 'digitization' of our society).</p>
<p>It turns out, that large, complex neural networks can take advantage of these huge data stores. Thus, we often say <em>scale</em> has been driving progress with deep learning, where scale means the size of the data, the size/complexity of the neural network, and the growth in computation.</p>
<p>The interplay between these 'scales' is apparent when you consider that many of the algorithmic advances of neural networks have come from making them more computational efficient.  </p>
<p><strong>Algorithmic Advances: ReLu</strong></p>
<p>One of the huge breakthroughs in neural networks has been the seemingly simple switch from the <strong>sigmoid</strong> activation function to the <strong>rectified linear</strong> (ReLu) activation function.</p>
<p>One of the problems with using <strong>sigmoid</strong> functions is that its gradients approach 0 as input to the sigmoid function approaches and \(+\infty\) and \(-\infty\) . In this case, the updates to the parameters become very small and our learning slows dramatically.</p>
<p>With ReLu units, our gradient is equal to \(1\) for all positive inputs. This makes learning with gradient descent much faster. See <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">here</a> for more information on ReLu's.</p>
<p><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/640px-Rectifier_and_softplus_functions.svg.png?1527957850905" /></p>
<p><strong>Scale Advances</strong></p>
<p>With smaller training sets, the relative ordering of the algorithms is actually not very well defined so if you don't have a lot of training data it is often up to your skill at hand engineering features that determines the
performance. For small training sets, it's quite possible that if someone training an SVM is more motivated to hand engineer features they will outperform a powerful neural network architecture.</p>
<p><a href="https://postimg.cc/image/t6w42u0sf/"><img alt="scale.png" src="https://s19.postimg.cc/6i6x39jer/scale.png" /></a></p>
<p>However, for very large training sets, <em>we consistently see large neural networks dominating the other approaches</em>.</p>
<h2 id="week-2-neural-networks-basics">Week 2: Neural networks basics</h2>
<h3 id="binary-classification">Binary Classification</h3>
<p>First, some notation,</p>
<ul>
<li>\(n\) is the number of data attributes, or <em>features</em></li>
<li>\(m\) is the number of input examples in our dataset (sometimes we write \(m_{train}, m_{test}\) to be more explicit).</li>
<li>our data is represented as input, output pairs, \((x^{(1)},y^{(1)}), ..., (x^{(m)},y^{(m)})\) where \(x \in \mathbb R^n\) , \(y \in {0,1}\)</li>
<li>\(X\) is our design matrix, which is simply columns of our input vectors \(x^{(i)}\) , thus it has dimensions of \(n\) x \(m\) .</li>
<li>\(Y = [y^{(1)}, ..., y^{(m)}]\) , and is thus a \(1\) x \(m\) matrix.</li>
</ul>
<blockquote>
<p>Note, this is different from many other courses which represent the design matrix, \(X\) as rows of transposed input vectors and the output vector \(Y\) as a \(m\) x \(1\) column vector. The above convention turns out to be easier to implement.</p>
</blockquote>
<p>When programming neural networks, implementation details become extremely important (<em>e.g</em>. vectorization in place of for loops).</p>
<p>We are going to introduce many of the key concepts of neural networks using <strong>logistic regression</strong>, as this will make them easier to understand. Logistic regression is an algorithm for <strong>binary classification</strong>. In binary classification, we have an input (<em>e.g</em>. an image) that we want to classifying as belonging to one of two classes.</p>
<h4 id="logistic-regression-crash-course">Logistic Regression (Crash course)</h4>
<p>Given an input feature vector \(x\) (perhaps corresponding to an images flattened pixel data), we want \(\hat y\) , the probability of the input examples class, \(\hat y = P(y=1 | x)\)</p>
<blockquote>
<p>If \(x\) is a picture, we want the chance that this is a picture of a cat, \(\hat y\) .</p>
</blockquote>
<p>The parameters of our model are \(w \in \mathbb R^{n_x}\) , \(b \in \mathbb R\) . Our output is \(\hat y = \sigma(w^Tx + b)\) were \(\sigma\) is the <strong>sigmoid function</strong>.</p>
<p><img alt="sigmoid" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png" /></p>
<p>The formula for the sigmoid function is given by: \(\sigma(z) = \frac{1}{1 + e^{-z}}\) where \(z = w^Tx + b\) . We notice a few things:</p>
<ul>
<li>If \(z\) is very large, \(e^{-z}\) will be close to \(0\) , and so \(\sigma(z)\) is very close to \(1\) .</li>
<li>If \(z\) is very small, \(e^{-z}\) will grow very large, and so \(\sigma(z)\) is very close to \(0\) .</li>
</ul>
<blockquote>
<p>It helps to look at the plot \(y = e^{-x}\)</p>
</blockquote>
<p>Thus, logistic regression attempts to learn parameters which will classify images based on their probability of belonging to one class or the other. The classification decision is decided by applying the sigmoid function to \(w^Tx + b\) .</p>
<blockquote>
<p>Note, with neural networks, it is easier to keep the weights \(w\) and the biases \(b\) separate. Another notation involves adding an extra parameters (\(w_0\) which plays the role of the bias.</p>
</blockquote>
<p><strong>Loss function</strong></p>
<p>Our prediction for a given example \(x^{(i)}\) is \(\hat y^{(i)} = \sigma(w^Tx^{(i)} + b)\) .</p>
<p>We chose <strong>loss function</strong>, \(\ell(\hat y, y) = -(y \; log\; \hat y + (1-y) \;log(1-\hat y))\).</p>
<p>We note that:</p>
<ul>
<li>If \(y=1\) , then the loss function is \(\ell(\hat y, y) = -log\; \hat y\) . Thus, the loss approaches zero as \(\hat y\) approaches 1.</li>
<li>If \(y=0\) , then the loss function is \(\ell(\hat y, y) = -log\; (1 -\hat y)\) . Thus, the loss approaches zero as \(\hat y\) approaches 0.</li>
</ul>
<blockquote>
<p>Note, while \(\ell_2\) loss is taught in many courses and seems like an appropriate choice, it is non-convex and so we cannot use gradient descent to optimize it.</p>
<p>An optional video is given further justifying the use of this loss function. Watch it and add notes here!</p>
</blockquote>
<p>Note that the <strong>loss function</strong> measures how well we are doing on a <em>single example</em>. We now define a <strong>cost function</strong>, which captures how well we are doing on the entire dataset:</p>
<p>\(J(w,b) = \frac{1}{m}\sum^m_{i=1} \ell(\hat y^{(i)}, y^{(i)}) = - \frac{1}{m}\sum^m_{i=1}(y^{(i)} \; log\; \hat y^{(i)} + (1-y^{(i)}) \;log(1-\hat y^{(i)}))\)</p>
<blockquote>
<p>Note that this notation is somewhat unique, typically the cost/loss functions are just interchangeable terms. However in this course, we will define the <strong>loss function</strong> as computing the error for a single training example and the <strong>cost function</strong> as the average of the loss functions of the entire training set.</p>
</blockquote>
<h4 id="gradient-descent">Gradient Descent</h4>
<p>We want to find \(w,b\) which minimize \(J(w,b)\) . We can plot the <strong>cost function</strong> with \(w\) and \(b\) as our horizontal axes:</p>
<p><a href="https://postimg.cc/image/a1suswm2n/"><img alt="cost_surface.png" src="https://s19.postimg.cc/pna6cuy0z/cost_surface.png" /></a></p>
<blockquote>
<p>In practice, \(w\) typically has many more dimensions.</p>
</blockquote>
<p>Thus, the cost function \(J(w,b)\) can be thought of as a surface, were the height of the surface above the horizontal axes is its value. We want to find the values of our parameters \(w, b\) at the lowest point of this surface, the point at which the average loss is at its minimum.</p>
<p><strong>Gradient Descent Algorithm</strong></p>
<p>Initialize \(w,b\) to some random values</p>
<blockquote>
<p>because this cost function is convex, it doesn't matter what values we use to initialize, \(0\) is usually chosen for logistic regression.</p>
</blockquote>
<p>Repeat</p>
<ol>
<li>\(w := w - \alpha \frac{dJ(w)}{dw}\)</li>
<li>\(b := b - \alpha \frac{dJ(w)}{db}\)</li>
</ol>
<blockquote>
<p>\(\alpha\) is our learning rate, it controls how big a step we take on each iteration. In some notations, we use \(\partial\) to denote the partial derivative of a function with \(2\) or more variables, and \(d\) to denote the derivative of a function of only \(1\) variable.</p>
</blockquote>
<p><a href="https://postimg.cc/image/y5jmh99pb/"><img alt="gradient_descent.png" src="https://s19.postimg.cc/a1susyr8j/gradient_descent.png" /></a></p>
<p>When implementing gradient descent in code, we will use the variable \(dw\) to represent \(\frac{dJ(w, b)}{dw}\) (this size of the step for \(w\) and \(db\) to represent \(\frac{dJ(w, b)}{db}\) (the size of the step for \(b\) .</p>
<h4 id="aside-calculus-review">(ASIDE) Calculus Review</h4>
<p><strong>Intuition about derivatives</strong></p>
<h6 id="linear-function-example">Linear Function Example</h6>
<p>Take the function \(f(a) = 3a\). Then \(f(a) = 6\) when \(a = 2\) . If we were to give \(a\) a tiny nudge, say to \(a = 2.001\) , what happens to \(f(a)\) ?</p>
<p><a href="https://postimg.cc/image/4qdy86sa7/"><img alt="derivative.png" src="https://s19.postimg.cc/wdqnmadgz/derivative.png" /></a></p>
<p>Then \(f(a) = 6.003\) , but more importantly if we inspect the triangle formed by performing the nudge, we can get the slope of the function between \(a\) and \(a + 0.001\) as the \(\frac{height}{width} = 3\) .</p>
<p>Thus, the <strong>derivative</strong> (or slope) of \(f(a)\) <em>w.r.t</em> \(a\) is \(3\) . We say that \(\frac{df(a)}{da} = 3\) or \(\frac{d}{da}f(a) = 3\)</p>
<blockquote>
<p>Add my calculus notes here!
Link to BlueBrown videos.</p>
</blockquote>
<h6 id="non-linear-function-example">Non-Linear Function Example</h6>
<p>Take the function \(f(a) = a^2\) . Then \(f(a) = 4\) when \(a = 2\) . If we were to give \(a\) a tiny nudge, say to \(a = 2.001\), what happens to \(f(a)\)?</p>
<p><a href="https://postimg.cc/image/89zvy3pvz/"><img alt="more_derivatives.png" src="https://s19.postimg.cc/535ceh5g3/more_derivatives.png" /></a></p>
<p>Then \(f(a) = 4.004\), but more importantly if we inspect the triangle formed by performing the nudge, we can get the slope of the function between \(a\) and \(a + 0.001\) as the \(\frac{height}{width} = 4\) .</p>
<p>In a similar way, we can perform this analysis for any point \(a\) on the plot, and we will see that slope of \(f(a)\) at some point \(a\) is equal to \(2a\) .</p>
<p>Thus, the <strong>derivative</strong> (or slope) of \(f(a)\) <em>w.r.t</em> \(a\) is \(2a\) . We say that \(\frac{df(a)}{da} = 2a\) or \(\frac{d}{da}f(a) = 2a\) .</p>
<h4 id="computation-graph">Computation Graph</h4>
<p>A <strong>computation graph</strong> organizes a series of computations into left-to-right and right-to-left passes. Lets build the intuition behind a computation graph.</p>
<p>Say we are trying to compute a function \(J(a,b,c) = 3(a + bc)\) . This computation of this function actually has three discrete steps:</p>
<ul>
<li>compute \(u = bc\)</li>
<li>compute \(v = a + u\)</li>
<li>compute J = \(3v\)</li>
</ul>
<p>We can draw this computation in a graph:</p>
<p><a href="https://postimg.cc/image/r2br1l6tr/"><img alt="computation_graph.png" src="https://s19.postimg.cc/qcsyp86ab/computation_graph.png" /></a></p>
<p>The computation graph is useful when you have some variable or output variable that you want to optimize (\(J\) in this case, in logistic regression it would be our <em>cost function output</em>). A <em>forward pass</em> through the graph is represented by <em>left-to-right</em> arrows (as drawn above) and a <em>backwards pass</em> is represented by <em>right-to-left</em> arrows.</p>
<p>A backwards pass is a natural way to represent the computation of our derivatives.  </p>
<p><strong>Derivatives with a computation graph</strong></p>
<p>Lets take a look at our computation graph, and see how we can use it to compute the partial derivatives of \(J\) i.e., lets carry out backpropogation on this computation graph by hand.</p>
<blockquote>
<p>Informally, you can think of this as asking: "If we were to change the value of \(v\) slightly, how would \(J\) change?"</p>
</blockquote>
<p><a href="https://postimg.cc/image/iwtp3evfj/"><img alt="clean_computation_graph.png" src="https://s19.postimg.cc/q01kj10v7/clean_computation_graph.png" /></a></p>
<p>First, we use our informal way of computing derivatives, and note that a small change to \(v\) results in a change to \(J\) of 3X that small change, and so \(\frac{dJ}{dv} = 3\) . This represents one step in our backward pass, the first step in backpropagation.</p>
<p>Now let's look at another example. What is \(\frac{dJ}{da}\)?</p>
<p>We compute the \(\frac{dJ}{da}\) from the second node in the computation graph by noting that a small change to \(a\) results in a change to \(J\) of 3X that small  change, and so \(\frac{dJ}{da} = 3.\) This represents our second step in our backpropagation.</p>
<p>One way to break this down is to say that by changing \(a\), we change \(v\), the magnitude of this change is \(\frac{dv}{da}\) . Through this change in \(v\), we also change \(J\), and the magnitude of the change is \(\frac{dJ}{dv}\) . To capture this more generally, we use the <strong>chain rule</strong> from calculus, informally:</p>
<p>\[\text{if } a \rightarrow v \rightarrow J \text{, then } \frac{dJ}{da} = \frac{dJ}{dv} \frac{dv}{da}\]</p>
<blockquote>
<p>Here, just take \(\rightarrow\) to mean 'effects'. A formal definition of the chain rule can be found <a href="https://en.wikipedia.org/wiki/Chain_rule">here</a>.</p>
</blockquote>
<p>The amount \(J\) changes when you when you nudge \(a\) is the product of the amount \(J\) changes when you nudge \(v\) multiplied by the amount \(v\) changes when you nudge \(a\) .</p>
<blockquote>
<p><strong>Implementation note</strong>: When writing code to implement backpropagation, there is typically a single output variable you want to optimize, \(dvar\), (the value of the cost function). We will follow to notation of calling this variable \(dvar\) .</p>
</blockquote>
<p>If we continue performing backpropagation steps, we can determine the individual contribution a change to the input variables has on the output variable. For example,</p>
<p>\[\frac{dJ}{db} = \frac{dJ}{du} \frac{du}{db} = (3)(2) = 6\]</p>
<p>The key take away from this video is that when computing derivatives to determine the contribution of input variables to change in an output variable, the most efficient way to do so is through a right to left pass through a computation graph. In particular, we'll first compute the derivative with respect to the output of the left-most node in a backward pass, which becomes useful for computing the derivative with respect to the next node and so forth. The <strong>chain rule</strong> makes the computation of these derivatives tractable.</p>
<h4 id="logistic-regression-gradient-descent">Logistic Regression Gradient Descent</h4>
<p>Logistic regression recap:</p>
<p>\[z = w^Tx + b\]
\[\hat y = a = \sigma(z)\]
\[\ell(a,y) = -(ylog(a) + (1-y)log(1-a))\]</p>
<blockquote>
<p>\(\ell\) is our loss for a single example, and \(\hat y\) are our predictions.</p>
</blockquote>
<p>For this example, lets assume we have only two features: \(x_1\), \(x_2\) . Our computation graph is thus:</p>
<p><a href="https://postimg.cc/image/le5gaphwv/"><img alt="computation_graph_logression.png" src="https://s19.postimg.cc/jz3vlzgtv/computation_graph_logression.png" /></a></p>
<p>Our goal is to modify the parameters to minimize the loss \(\ell\) . This translates to computing derivatives \(w.r.t\) the loss function. Following our generic example above, we can compute all the relevant derivatives using the chain rule. The first two passes are computed by the following derivatives:</p>
<ol>
<li>\(\frac{d\ell(a,y)}{da} = - \frac{y}{a} + \frac{1-y}{1-a}\)</li>
<li>\(\frac{d\ell(a,y)}{dz} = \frac{d\ell(a,y)}{da} \cdot \frac{da}{dz} =  a - y\)</li>
</ol>
<blockquote>
<p>Note: You should prove these to yourself.</p>
<p><strong>Implementation note</strong>, we use \(dx\) as a shorthand for \(\frac{d\ell(\hat y,y)}{dx}\) for some variable \(x\) when implementing this in code.</p>
</blockquote>
<p>Recall that the final step is to determine the derivatives of the loss function \(w.r.t\) to the parameters.</p>
<ul>
<li>\(\frac{d\ell(a,y)}{dw_1} = x_1 \cdot \frac{d\ell(a,y)}{dz}\)</li>
<li>\(\frac{d\ell(a,y)}{dw_2} = x_2 \cdot \frac{d\ell(a,y)}{dz}\)</li>
</ul>
<p>One step of gradient descent would perform the updates:</p>
<ul>
<li>\(w_1 := w_1 - \alpha \frac{d\ell(a,y)}{dw_1}\)</li>
<li>\(w_2 := w_2 - \alpha \frac{d\ell(a,y)}{dw_2}\)</li>
<li>\(b := b - \alpha \frac{d\ell(a,y)}{db}\)</li>
</ul>
<p><strong>Extending to \(m\) examples</strong></p>
<p>Lets first remind ourself of the logistic regression <strong>cost</strong> function:</p>
<p>\[J(w,b) = \frac{1}{m}\sum^m_{i=1} \ell(\hat y^{(i)}, y^{(i)}) = - \frac{1}{m}\sum^m_{i=1}(y^{(i)} \; log\; \hat y^{(i)} + (1-y^{(i)}) \;log(1-\hat y^{(i)}))\]</p>
<p>Where,</p>
<p>\[\hat y = a = \sigma(z) = \sigma(w^Tx^{(i)} + b)\]</p>
<p>In the example above for a single training example, we showed that to perform a gradient step we first need to compute the derivatives \(\frac{d\ell(a,y)}{dw_1}, \frac{d\ell(a,y)}{dw_2}, \frac{d\ell(a,y)}{db}\) . For \(m\) examples, these are computed as follows:</p>
<ul>
<li>\(\frac{\partial\ell(a,y)}{\partial dw_1} = \frac{1}{m}\sum^m_{i=1} \frac{\partial}{\partial w_1} \ell(\hat y^{(i)}, y^{(i)})\)</li>
<li>\(\frac{\partial\ell(a,y)}{\partial w_2} = \frac{1}{m}\sum^m_{i=1} \frac{\partial}{\partial w_2} \ell(\hat y^{(i)}, y^{(i)})\)</li>
<li>\(\frac{\partial\ell(a,y)}{\partial b} = \frac{1}{m}\sum^m_{i=1} \frac{\partial}{\partial b} \ell(\hat y^{(i)}, y^{(i)})\)</li>
</ul>
<p>We have already shown on the previous slide how to compute \(\frac{\partial}{\partial w_1} \ell(\hat y^{(i)}, y^{(i)}), \frac{\partial}{\partial w_2} \ell(\hat y^{(i)}, y^{(i)})\) and \(\frac{\partial}{\partial b} \ell(\hat y^{(i)}, y^{(i)})\) . Gradient descent for \(m\) examples essentially involves computing these derivatives for each input example \(x^{(i)}\) and averaging the result before performing the gradient step. Concretely, the pseudo-code for gradient descent on \(m\) examples of \(n=2\) features follows:</p>
<p><strong>ALGO</strong></p>
<p>Initialize \(J=0; dw_1 = 0; dw_2 = 0; db = 0\)</p>
<p>for \(i=1\) to \(m\):</p>
<ul>
<li>\(z^{(i)} = w^Tx^{(i)}\)</li>
<li>\(a^{(i)} = \sigma(z^{(i)})\)</li>
<li>\(J \text{+= } -[y^{(i)}log(a^{(i)}) + (1-y^{(i)})log(1-a^{(i)})]\)</li>
<li>\(dz^{(i)} = a^{(i)} - y^{(i)}\)</li>
<li>for \(j = 1\) to \(n\)</li>
<li>\(dw_j \text{+= } x_j^{(i)}dz^{(i)}\)</li>
<li>\(dw_j \text{+= } x_j^{(i)}dz^{(i)}\)</li>
<li>\(db \text{+= } dz^{(i)}\)</li>
</ul>
<p>\(J = J/m;\; dw_1 \text{=/ } m;\; dw_2 \text{=/ }  m;\;b \text{=/ }  m\)</p>
<p>In plain english, for each training example, we use the sigmoid function to compute its activation, accumulate a loss for that example based on the current parameters, compute the derivative of the current cost function \(w.r.t\) the activation function, and update our parameters and bias. Finally we take the average of our cost function and our gradients.</p>
<p>Finally, we use our derivatives to update our parameters,</p>
<ul>
<li>\(w_1 := w_1 - \alpha \cdot {dw_1}\)</li>
<li>\(w_2 := w_2 - \alpha \cdot {dw_2}\)</li>
<li>\(b := b - \alpha \cdot {db}\)</li>
</ul>
<p>This constitutes <strong>one step</strong> of gradient descent.</p>
<p>The main problem with this implementation is the nested for loops. For deep learning, which requires very large training sets, <em>explicit for loops</em> will make our implementation very slow. Vectorizing this algorithm will greatly speed up our algorithms running time.</p>
<h4 id="vectorization">Vectorization</h4>
<p>Vectorization is basically the art of getting rid of explicit for loops. In practice, deep learning requires large datasets (at least to obtain high performance). Explicit for loops lead to computational overhead that significantly slows down the training process.</p>
<p>The main reason vectorization makes such a dramatic difference is that it allows us to take advantage of <strong>parallelization</strong>. The rule of thumb to remember is: <em>whenever possible, avoid explicit for-loops</em>.</p>
<blockquote>
<p>In a toy example were \(n_x\) is \(10^6\), and \(w, x^{(i)}\) are random values, vectorization leads to an approximately 300X speed up to compute all \(z^{(i)}\)</p>
</blockquote>
<p>Lets take a look at some explicit examples:</p>
<ul>
<li>Multiply a <strong>matrix</strong> by a <strong>vector</strong>, e.g., \(u = Av\) .</li>
</ul>
<p>So, \(u_i = \sum_jA_{ij}v_j\) . Instead of using for nested loops, use: <code>u = np.dot(A,v)</code></p>
<ul>
<li>Apply exponential operation on every element of a matrix/vector \(v\).</li>
</ul>
<p>Again, use libraries such as <code>numpy</code> to perform this with a single operation, e.g., <code>u = np.exp(v)</code></p>
<blockquote>
<p>This example applies to almost all operations, <code>np.log(v)</code>, <code>np.abs(v)</code>, <code>np.max(v)</code>, etc...</p>
</blockquote>
<p><strong>Example: Vectorization of Logistic Regression</strong></p>
<h6 id="forward-pass">Forward pass</h6>
<p>Lets first review the forward pass of logistic regression for \(m\) examples:</p>
<p>\(z^{(1)} = w^Tx^{(1)} + b\);  \(a^{(1)} = \sigma(z^{1})\), \(...\) , \(z^{(m)} = w^Tx^{(m)} + b\);  \(a^{(m)} = \sigma(z^{m})\)</p>
<p>In logistic regression, we need to compute \(z^{(i)} = w^Tx^{(i)}+b\) for each input example \(x^{(i)}\) . Instead of using a for loop over each \(i\) in range \((m)\) we can use a vectorized implementation to compute z directly.</p>
<p>Our vectors are of the dimensions: \(w \in \mathbb R^{n_x}\), \(b \in \mathbb R^{n_x}\), \(x \in \mathbb R^{n_x}\).</p>
<p>Our parameter vector, bias vector, and design matrix are,</p>
<p>\(w = \begin{bmatrix}w_1 \\ ... \\ w_{n_x}\end{bmatrix}\), \(b = \begin{bmatrix}b_1 \\ ... \\ b_{n_x}\end{bmatrix}\), \(X = \begin{bmatrix}x^{(1)}<em>1 &amp; ... &amp; x^{(m)} \\ ... \\ x^{(1)}</em>{n_x}\end{bmatrix}\)</p>
<p>So, \(w^T \cdot X + b = w^Tx^{(i)} + b\) (for all \(i\)). Thus we can compute all \(w^Tx^{(i)}\) in one operation if we vectorize!</p>
<p>In numpy code:</p>
<p><code>Z = np.dot(w.T, X) + b</code></p>
<blockquote>
<p>Note, \(+ b\) will perform element-wise addition in python, and is an example of <strong>broadcasting</strong>.</p>
</blockquote>
<p>Where \(Z\) is a row vector \([z^{(1)}, ..., z^{(m)}]\) .</p>
<h6 id="backward-pass">Backward pass</h6>
<p>Recall, for the gradient computation, we computed the following derivatives:</p>
<p>\(dz^{(1)} = a^{(1)} - y^{(1)} ... dz^{(m)} = a^{(m)} - y^{(m)}\)</p>
<p>We define a row vector,</p>
<p>\(dZ = [dz^{(1)}, ..., dz^{(m)}]\) .</p>
<p>From which it is trivial to see that,</p>
<p>\(dZ = A - Y\), where \(A = [a^{(1)}, ..., a^{(m)}]\) and \(Y = [y^{(1)}, ..., y^{(m)}]\) .</p>
<blockquote>
<p>This is an element-wise subtraction, \(a^{(1)} - y^{(1)}, ..., a^{(m)} - y^{(m)}\) that produces a \(m\) length row vector.</p>
</blockquote>
<p>We can then compute our <em>average</em> derivatives of the cost function \(w.r.t\) to the parameters in two lines of codes,</p>
<p><code>db = 1/m * np.sum(dZ)</code></p>
<p><code>dw = 1/m * np.dot(X, dZ.T)</code></p>
<p>Finally, we compare our non-vectorized approach to linear regression vs our vectorized approaches</p>
<table>
<thead>
<tr>
<th align="center">Non-vectorized Approach</th>
<th align="center">Vectorized Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://postimg.cc/image/shdbqdksf/"><img alt="gd_no_vectorization.png" src="https://s19.postimg.cc/w0z9g6nib/gd_no_vectorization.png" /></a></td>
<td align="center"><a href="https://postimg.cc/image/u96al9wfj/"><img alt="gd_vectorization.png" src="https://s19.postimg.cc/pna6cxawj/gd_vectorization.png" /></a></td>
</tr>
<tr>
<td align="center">Two for loops, one over the training examples \(x^{(i)}\) and a second over the features \(x^{(i)}_j\) . We have omitted the outermost loop that iterates over gradient steps.</td>
<td align="center">Note that, we still need a single for loop to iterate over each gradient step (regardless if we are using stochastic or mini-batch gradient descent) even in our vectorized approach.</td>
</tr>
</tbody>
</table>
<h4 id="broadcasting">Broadcasting</h4>
<p>Lets motivate the usefulness of <strong>broadcasting</strong> with an example. Lets say you wanted to get the percent of total calories from carbs, proteins, and fats for multiple foods.</p>
<p><a href="https://postimg.cc/image/le5gapa73/"><img alt="food_matrix.png" src="https://s19.postimg.cc/q01kj1vqb/food_matrix.png" /></a></p>
<p><em>Can we do this without an explicit for loop?</em></p>
<p>Set this matrix to a <code>(3,4)</code> numpy matrix <code>A</code>.</p>
<div class="highlight"><pre><span></span>import numy as np

# some numpy array of shape (3,4)
A = np.array([
  [...],
  [...],
  [...]
  ])

cal = A.sum(axis=0) # get column-wise sums
percentage = 100 * A / cal.reshape(1,4) # get percentage of total calories
</pre></div>

<p>So, we took a <code>(3,4)</code> matrix <code>A</code> and divided it by a <code>(1,4)</code> matrix <code>cal</code>. This is an example of broadcasting.</p>
<p>The general principle of broadcast can be summed up as follows:</p>
<ul>
<li>\((m,n) \text{ [+ OR - OR * OR /] } (1, n) \Rightarrow (m,n) \text{ [+ OR - OR * OR /] } (m \text{ copies}, n)\)</li>
<li>\((m,n) \text{ [+ OR - OR * OR /] } (m, 1) \Rightarrow (m,n) \text{ [+ OR - OR * OR /] } (m, n \text{ copies})\)</li>
</ul>
<p>Where \((m, n), (1, n)\) are matrices, and the operations are performed <em>element-wise</em> after broadcasting.</p>
<p><strong>More broadcasting examples</strong></p>
<h6 id="addition">Addition</h6>
<p><em>Example 1</em>: \(\begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + 100 == \begin{bmatrix}1 \\ 2 \\ 3 \\ 4\end{bmatrix} + \begin{bmatrix}100 \\ 100 \\ 100 \\ 100\end{bmatrix} = \begin{bmatrix}101 \\ 102 \\ 103 \\ 104\end{bmatrix}\)</p>
<p><em>Example 2</em>: \(\begin{bmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{bmatrix} + \begin{bmatrix}100 &amp; 200 &amp; 300\end{bmatrix} == \begin{bmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{bmatrix} + \begin{bmatrix}100 &amp; 200 &amp; 300 \\ 100 &amp; 200 &amp; 300\end{bmatrix} = \begin{bmatrix}101 &amp; 202 &amp; 303 \\ 104 &amp; 205 &amp; 306\end{bmatrix}\)</p>
<p><em>Example 3</em>: \(\begin{bmatrix}1 &amp; 2 &amp; 3  \\ 4 &amp; 5 &amp; 6\end{bmatrix} + \begin{bmatrix}100 \\ 200\end{bmatrix} == \begin{bmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{bmatrix} + \begin{bmatrix}100 &amp; 100 &amp; 100 \\ 200 &amp; 200 &amp; 200\end{bmatrix} = \begin{bmatrix}101 &amp; 202 &amp; 303 \\ 104 &amp; 205 &amp; 206\end{bmatrix}\)</p>
<h4 id="aisde-a-note-on-pythonnumpy-vectors">(AISDE) A note on python/numpy vectors</h4>
<p>The great flexibility of the python language paired with the numpy library is both a strength and a weakness. It is a strength because of the great expressivity of the pair, but with this comes the opportunity to intro strange, hard-to-catch bugs if you aren't familiar with the intricacies of numpy and in particular broadcasting.</p>
<p>Here are a couple of tips and tricks to minimize the number of these bugs:</p>
<ul>
<li>Creating a random array: <code>a = np.random.randn(5)</code></li>
<li>Arrays of shape <code>(x, )</code> are known as <strong>rank 1 array</strong>. They have some nonintuitive properties and don't consistently behave like either a column vector or a row vector. Let <code>b</code> be a rank 1 array.</li>
<li><code>b.T == b</code></li>
<li><code>np.dot(b, b.T)</code> is a real number, <em>not the outer product as you might expect</em>.</li>
<li>Thus, in this class at least, using rank 1 tensors with an unspecified dimension length is not generally advised. <em>Always specify both dimensions</em>.</li>
<li>If you know the size that your numpy arrays should be in advance, its often useful to throw in a python assertion to help catch strange bugs before they happen:</li>
<li><code>assert(a.shape == (5,1))</code></li>
<li>Additionally, the reshape function runs in linear time and is thus very cheap to call, use it freely!</li>
<li><code>a = a.reshape((5,1))</code></li>
</ul>
<h2 id="week-3-shallow-neural-networks">Week 3: Shallow neural networks</h2>
<h3 id="neural-network-overview">Neural network overview</h3>
<p>Up until this point, we have used logistic regression as a stand-in for neural networks. The "network" we have been describing looked like:</p>
<table>
<thead>
<tr>
<th align="center">Network</th>
<th align="center">Computation Graph</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://postimg.cc/image/jmchfu8un/"><img alt="lr_overview.png" src="https://s19.postimg.cc/3o3rppemr/lr_overview.png" /></a></td>
<td align="center"><a href="https://postimg.cc/image/cj4m09dpr/"><img alt="lr_overview_graph.png" src="https://s19.postimg.cc/mgfmtblbn/lr_overview_graph.png" /></a></td>
</tr>
</tbody>
</table>
<blockquote>
<p>\(a\) and  \(\hat y\) are used interchangeably</p>
</blockquote>
<p>A neural network looks something like this:</p>
<table>
<thead>
<tr>
<th align="center">Network</th>
<th align="center">Computation Graph</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://postimg.cc/image/c6d7u4dgf/"><img alt="nn_overview.png" src="https://s19.postimg.cc/77ppfl9nn/nn_overview.png" /></a></td>
<td align="center"><a href="https://postimg.cc/image/535cehkvj/"><img alt="nn_overview_graph.png" src="https://s19.postimg.cc/mt70ziygj/nn_overview_graph.png" /></a></td>
</tr>
</tbody>
</table>
<blockquote>
<p>We typically don't distinguish between \(z\) and \(a\) when talking about neural networks, one neuron = one activation = one \(a\) like calculation.</p>
</blockquote>
<p>We will introduce the notation of superscripting values with \(^{[l]}\), where \(l\) refers to the layer of the neural network that we are talking about.</p>
<blockquote>
<p>Not to be confused with \(^{(i)}\) which we use to refer to a single input example \(i\) .</p>
</blockquote>
<p><em>The key intuition is that neural networks stack activations of inputs multiplied by their weights</em>.</p>
<p>Similar to the 'backwards' step that we discussed for logistic regression, we will explore the backwards steps that makes learning in a neural network possible.</p>
<h4 id="neural-network-representation">Neural network Representation</h4>
<p>This is the canonical representation of a neural network</p>
<p><a href="https://postimg.cc/image/4qdy8babj/"><img alt="neural_network_basics.png" src="https://s19.postimg.cc/vbgh3vcoz/neural_network_basics.png" /></a></p>
<p>On the left, we have the <strong>input features</strong> stacked vertically. This constitutes our <strong>input layer</strong>. The final layer, is called the <strong>output layer</strong> and it is responsible for generating the predicted value \(\hat y\) . Any layer in between these two layers is known as a <strong>hidden layer</strong>. This name derives from the fact that the <em>true values</em> of these hidden units is not observed in the training set.</p>
<blockquote>
<p>The hidden layers and output layers have parameters associated with them. These parameters are denoted \(W^{[l]}\) and \(b^{[l]}\) for layer \(l\) .</p>
</blockquote>
<p>Previously, we were referring to our input examples as \(x^{(i)}\) and organizing them in a design matrix \(X\) . With neural networks, we will introduce the convention of denoting output values of a layer \(l\), as a column vector \(a^{[l]}\), where \(a\) stands for <em>activation</em>. You can also think of these as the values a layer \(l\) passes on to the next layer.</p>
<p>Another note: the network shown above is a <em>2-layer</em> neural network. We typically do not count the input layer. In light of this, we usually denote the input layer as \(l=0\).</p>
<h4 id="computing-a-neural-networks-output">Computing a Neural Networks Output</h4>
<p>We will use the example of a single hidden layer neural network to demonstrate the forward propagation of inputs through the network leading to the networks output.</p>
<p>We can think of each unit in the neural network as performing two steps, the <em>multiplication of inputs by weights and the addition of a bias</em>, and the <em>activation of the resulting value</em></p>
<p><a href="https://postimg.cc/image/i8kuk2z67/"><img alt="unit_breakdown.png" src="https://s19.postimg.cc/qquaof5oz/unit_breakdown.png" /></a></p>
<blockquote>
<p>Recall, that we will use a superscript, \(^{[l]}\) to denote values belonging to the \(l-th\) layer.</p>
</blockquote>
<p>So, the \(j^{th}\) node of the \(l^{th}\) layer performs the computation</p>
<p>\[ a_j^{[l]} = \sigma(w_j^{[l]^T}a^{[l-1]} + b_j^{[l]}) \]</p>
<blockquote>
<p>Where \(a^{[l-1]}\) is the activation values from the precious layer.</p>
</blockquote>
<p>for some input \(x\). With this notation, we can draw our neural network as follows:</p>
<p><a href="https://postimg.cc/image/f0gd7lxn3/"><img alt="new_notation_nn.png" src="https://s19.postimg.cc/6i6x39r4j/new_notation_nn.png" /></a></p>
<p>In order to easily vectorize the computations we need to perform, we designate a matrix \(W^{[l]}\) for each layer \(l\), which has dimensions <em>(number of units in current layer X number of units in previous layer)</em></p>
<p>We can vectorize the computation of \(z^{[l]}\) as follows:</p>
<p><a href="https://postimg.cc/image/66pgpz08f/"><img alt="vectorized_z_nn.png" src="https://s19.postimg.cc/n78cynd9v/vectorized_z_nn.png" /></a></p>
<p>And the computation of \(a^{[l]}\) just becomes the element-wise application of the sigmoid function:</p>
<p><a href="https://postimg.cc/image/n78cymaov/"><img alt="vectorized_a_nn.png" src="https://s19.postimg.cc/7yifkuh0j/vectorized_a_nn.png" /></a></p>
<p>We can put it all together for our two layer neural network, and outline all the computations using our new notation:</p>
<p><a href="https://postimg.cc/image/h50q8noz3/"><img alt="putting_it_all_together_new_notation.png" src="https://s19.postimg.cc/5so4qvgab/putting_it_all_together_new_notation.png" /></a></p>
<h4 id="vectorizing-across-multiple-examples">Vectorizing across multiple examples</h4>
<p>In the last video, we saw how to compute the prediction for a neural network with a single input example. In this video, we introduce a vectorized approach to compute predictions for many input examples.  </p>
<p>We have seen how to take a single input example \(x\) and compute \(a^{[2]} = \hat y\) for a 2-layered neural network. If we have \(m\) training examples, we can used a vectorized approach to compute all \(m\) predictions.</p>
<p>First, lets introduce a new notation. The activation values of layer \(l\) for input example \(i\) is:</p>
<p>\[ a^{<a href="i">l</a>} \]</p>
<p>The \(m\) predictions our 2-layered are therefore computed in the following way:</p>
<p><a href="https://postimg.cc/image/i7awr5acf/"><img alt="m_examples_nn.png" src="https://s19.postimg.cc/mt70zhvvn/m_examples_nn.png" /></a></p>
<p>Recall that \(X\) is a \((n_x, m)\) design matrix, where each column is a single input example and \(W^{[l]}\) is a matrix where each row is the transpose of the parameter column vector for layer \(l\).</p>
<p>Thus, we can now compute the activation of a layer in the neural network for all training examples:</p>
<p>\[Z^{[l]} = W^{[l]}X + b^{[l]}\]
\[A^{[l]} = sign(Z^{[l]})\]</p>
<p>As an example, the result of a matrix multiplication of \(W^{[1]}\) by \(X\) is a matrix with dimensions \((j, m)\) where \(j\) is the number of units in layer \(1\) and \(m\) is the number of input examples</p>
<p><a href="https://postimg.cc/image/un7mkfljj/"><img alt="WX_vector.jpg" src="https://s19.postimg.cc/6w892blcj/WX_vector.jpg" /></a></p>
<p>\(A^{[l]}\) is therefore a matrix of dimensions (size of layer \(l\) X \(m\)). The top-leftmost value is the activation for the first unit in the layer \(l\) for the first input example \(i\), and the bottom-rightmost value is the activation for the last unit in the layer \(l\) for the last input example \(m\) .</p>
<p><a href="https://postimg.cc/image/o9ijh617z/"><img alt="vectorized_activations.png" src="https://s19.postimg.cc/dmoqbqt2r/vectorized_activations.png" /></a></p>
<h3 id="activation-functions">Activation Functions</h3>
<p>So far, we have been using the <strong>sigmoid</strong> activation function</p>
<p>\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</p>
<p>It turns out there are much better options.</p>
<h4 id="tanh">Tanh</h4>
<p>The <strong>hyperbolic tangent function</strong> is a non-linear activation function that almost always works better than the sigmoid function.</p>
<p>\[tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</p>
<p><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Sinh_cosh_tanh.svg/640px-Sinh_cosh_tanh.svg.png?1514655794955.png" /></p>
<blockquote>
<p>The tanh function is really just a shift of the sigmoid function so that it crosses through the origin.</p>
</blockquote>
<p>The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.</p>
<p>The single exception of sigmoid outperforming tanh is when its used in the ouput layer. In this case, it can be more desirable to scale our outputs from \(0\) to \(1\) (particularly in classification, when we want to output the probability that something belongs to a certain class). Indeed, we often mix activation functions in neural networks, and denote them:</p>
<p>\[g^{[p]}(z)\]</p>
<p>Where \(p\) is the \(p^{th}\) activation function.</p>
<p>If \(z\) is either very large, or very small, the derivative of both the tanh and sigmoid functions becomes very small, and this can slow down learning.</p>
<h4 id="relu">ReLu</h4>
<p>The <strong>rectified linear unit</strong> activation function solves the disappearing gradient problem faced by tanh and sigmoid activation functions. In practice, it also leads to faster learning.</p>
<p>\[ReLu(z) = max(0, z)\]</p>
<p><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/640px-Rectifier_and_softplus_functions.svg.png?1528644452536" /></p>
<blockquote>
<p>Note: the derivative at exactly 0 is not well-defined. In practice, we can simply set it to 0 or 1 (it matters little, due to the unlikeliness of a floating point number to ever be \(0.0000...\) exactly).</p>
</blockquote>
<p>One disadvantage of ReLu is that the derivative is equal to \(0\) when \(z\) is negative. <strong>Leaky ReLu</strong>'s aim to solve this problem with a slight negative slope for values of \(z&lt;0\) .</p>
<p>\[ReLu(z) = max(0.01 * z, z)\]</p>
<p><img alt="" src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/leaky.png" /></p>
<blockquote>
<p>Image sourced from <a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/leaky.png">here</a>.</p>
</blockquote>
<p>Sometimes, the \(0.01\) value is treated as an adaptive parameter of the learning algorithm. Leaky ReLu's solve a more general problem of "<a href="https://www.quora.com/What-is-the-definition-of-a-dead-neuron-in-Artificial-Neural-Networks?share=1">dead neurons</a>". However, it is not used as much in practice.</p>
<p><strong>Rules of thumb for choosing activations functions</strong></p>
<ul>
<li><em>If your output is a 0/1 value</em>, i.e., you are performing binary classification, the sigmoid activation is a natural choice for the output layer.</li>
<li><em>For all other units</em>, ReLu's is increasingly the default choice of activation function.</li>
</ul>
<p><strong>Why do you need non-linear activation functions?</strong></p>
<p>We could imagine using some <strong>linear</strong> activation function, \(g(z) = z\) in place of the <strong>non-linear</strong> activation functions we have been using so far. Why is this a bad idea? Lets illustrate out explanation using our simple neural networks</p>
<p><a href="https://postimg.cc/image/4qdy8babj/"><img alt="neural_network_basics.png" src="https://s19.postimg.cc/vbgh3vcoz/neural_network_basics.png" /></a></p>
<p>For this linear activation function, the activations of our simple network become:</p>
<p>\[z^{[1]} = W^{[1]}x + b^{[1]}\]
\[a^{[1]} = z^{[1]}\]
\[z^{[2]} = W^{[2]}x + b^{[2]}\]
\[a^{[2]} = z^{[2]}\]</p>
<p>From which we can show that,</p>
<p>\[a^{[2]} = (W^{[2]}W^{[1]})x + (W^{[2]}b^{[1]} + b^{[2]})\]
\[a^{[2]} = W'x + b' \text{, where } W' = W^{[2]}W^{[1]} \text{ and } b' = W^{[2]}b^{[1]} + b^{[2]}\]</p>
<p>Therefore, in the case of a <em>linear activation function</em>, the neural network is outputting a <em>linear function of the inputs</em>, no matter how many hidden layers!</p>
<h6 id="exceptions">Exceptions</h6>
<p>There are (maybe) two cases in which you may actually want to use a linear activation function.</p>
<ol>
<li>The output layer of a network used to perform regression, where we want \(\hat y\) to be a real-valued number, \(\hat y \in \mathbb R\)</li>
<li>Extremely specific cases pertaining to compression.</li>
</ol>
<h4 id="derivatives-of-activation-functions">Derivatives of activation functions</h4>
<p>When performing back-propogation on a network, we need to compute the derivatives of the activation functions. Lets take a look at our activation functions and their derivatives</p>
<p><strong>Sigmoid</strong></p>
<p><a href="https://postimg.cc/image/535ceiv67/"><img alt="sigmoid_deriv.png" src="https://s19.postimg.cc/dy66p1jyr/sigmoid_deriv.png" /></a></p>
<p>The deriviative of \(g(z)\), \(g(z)'\) is:</p>
<p>\[\frac{d}{dz}g(z) = \frac{1}{1 + e^{-z}}(1 - \frac{1}{1 + e^{-z}})= g(z)(1-g(z)) = a(1-a)\]</p>
<blockquote>
<p>We can sanity check this by inputting very large, or very small values of \(z\) into our derivative formula and inspecting the size of the outputs.</p>
</blockquote>
<p>Notice that if we have already computed the value of \(a\), we can very cheaply compute the value of \(g(z)'\) .</p>
<p><strong>Tanh</strong></p>
<p><a href="https://postimg.cc/image/i7awr82nj/"><img alt="tanh_deriv.png" src="https://s19.postimg.cc/g2qjq510z/tanh_deriv.png" /></a></p>
<p>The deriviative of \(g(z)\), \(g(z)'\) is:</p>
<p>\[\frac{d}{dz}g(z) = 1 - (tanh(z))^z\]</p>
<blockquote>
<p>Again, we can sanity check this inspecting that the outputs for different values of \(z\) match our intuition about the activation function.</p>
</blockquote>
<p><strong>ReLu</strong></p>
<p><a href="https://postimg.cc/image/iwtp3jswf/"><img alt="relu_deriv.png" src="https://s19.postimg.cc/i7awr6scz/relu_deriv.png" /></a></p>
<p>The derivative of \(g(z)\), \(g(z)'\) is:</p>
<p>\[\frac{d}{dz}g(z) = 0 \text{ if } z &lt; 0 ; 1 \text{ if } z &gt; 0; \text{ undefined if } z = 0\]</p>
<blockquote>
<p>If \(z = 0\), we typically default to setting \(g(z)\) to either \(0\) or \(1\) . In practice this matters little.</p>
</blockquote>
<h3 id="gradient-descent-for-neural-networks">Gradient descent for Neural Networks</h3>
<p>Lets implement gradient descent for our simple 2-layer neural network.</p>
<p>Recall, our parameters are: \(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\) . We have number of features, \(n_x = n^{[0]}\), number of hidden units \(n^{[1]}\), and \(n^{[2]}\) output units.</p>
<p>Thus our dimensions:</p>
<ul>
<li>\(W^{[1]}\) : (\(n^{[1]}, n^{[0]}\))</li>
<li>\(b^{[1]}\) : (\(n^{[1]}, 1\))</li>
<li>\(W^{[2]}\) : (\(n^{[2]}, n^{[1]}\))</li>
<li>\(b^{[2]}\) : (\(n^{[2]}, 1\))</li>
</ul>
<p>Our cost function is: \(J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^m \ell(\hat y, y)\)</p>
<blockquote>
<p>We are assuming binary classification.</p>
</blockquote>
<p><strong>Gradient Descent sketch</strong></p>
<ol>
<li>Initialize parameters <em>randomly</em></li>
<li>Repeat:<ul>
<li>compute predictions \(\hat y^{(i)}\) for \(i = 1 ,..., m\)</li>
<li>\(dW^{[1]} = \frac{\partial J}{\partial W^{[1]}}, db^{[1]} = \frac{\partial J}{\partial b^{[1]}}, ...\)</li>
<li>\(W^{[1]} = W^{[1]} - \alpha dW^{[1]}, ...\)</li>
<li>\(b^{[1]} = b^{[1]} - \alpha db^{[1]}, ...\)</li>
</ul>
</li>
</ol>
<p>The key to gradient descent is to computation of the derivatives, \(\frac{\partial J}{\partial W^{[l]}}\) and \(\frac{\partial J}{\partial b^{[l]}}\) for all layers \(l\) .</p>
<h4 id="formulas-for-computing-derivatives">Formulas for computing derivatives</h4>
<p>We are going to simply present the formulas you need, and defer their explanation to the next video. Recall the computation graph for our 2-layered neural network:</p>
<p><a href="https://postimg.cc/image/535cehkvj/"><img alt="nn_overview_graph.png" src="https://s19.postimg.cc/mt70ziygj/nn_overview_graph.png" /></a>|</p>
<p>And the vectorized implementation of our computations in our <strong>forward propagation</strong></p>
<p>1.\[Z^{[1]} = W^{[1]}X + b^{[1]}\]
2.\[A^{[1]} = g^{[1]}(z^{[1]})\]
3.\[Z^{[2]} = W^{[2]}X + b^{[2]}\]
4.\ß[A^{[2]} = g^{[2]}(z^{[2]})\]</p>
<blockquote>
<p>Where \(g^{[2]}\) would likely be the sigmoid function if we are doing binary classification.</p>
</blockquote>
<p>Now we list the computations for our <strong>backward propagation</strong></p>
<p>1.\[ dZ^{[2]} = A^{[2]} - Y \]
2.\[ dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T} \]</p>
<blockquote>
<p>Transpose of A accounts for the fact that W is composed of transposed column vectors of parameters.</p>
</blockquote>
<p>3.\[db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims=True)\]</p>
<blockquote>
<p>Where \(Y = [y^{(1)}, ..., y^{[m]}]\) . The <code>keepdims</code> arguments prevents numpy from returning a rank 1 array, \((n,)\)</p>
</blockquote>
<p>4.\[dZ^{[1]} = W^{[2]T}dZ^{[2]} \odot g(Z)' (Z^{[1]})\]</p>
<blockquote>
<p>Where \(\odot\) is the element-wise product. Note: this is a collapse of \(dZ\) and \(dA\) computations.</p>
</blockquote>
<p>5.\[dW{[1]} = \frac{1}{m} = dZ^{[1]}X^T\]
6.\[db^{[1]} = \frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)\]</p>
<h3 id="random-initialization">Random Initialization</h3>
<p>When you train your neural network, it is important to initialize your parameters <em>randomly</em>. With logistic regression, we were able to initialize our weights to <em>zero</em> because the cost function was convex. We will see that this <em>will not work</em> with neural networks.</p>
<p>Lets take the following network as example:</p>
<p><a href="https://postimg.cc/image/aslkya3r3/"><img alt="super_simple_network.png" src="https://s19.postimg.cc/8b9tr0jur/super_simple_network.png" /></a></p>
<p>Lets say we initialize our parameters as follows:</p>
<p>\(W^{[1]} = \begin{bmatrix}0 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\), \(b^{[1]} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\),
\(W^{[2]} = \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\),
\(b^{[2]} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</p>
<blockquote>
<p>It turns out that initializing the bias \(b\) with zeros is OK.</p>
</blockquote>
<p>The problem with this initialization is that for any input examples \(i, j\),</p>
<p>\[a^{[1]}_i == a^{[1]}_j\]</p>
<p>Similarly,</p>
<p>\[dz^{[1]}_i == dz^{[1]}_j\]</p>
<p>Thus, \(dW^{[1]}\) will be some matrix \(\begin{bmatrix}u &amp; v \\ u &amp; v\end{bmatrix}\) and all updates to the parameters \(W^{[1]}\) will be identical.</p>
<blockquote>
<p>Note we are referring to our single hidden layer \(^{[1]}\) but this would apply to any hidden layer of any fully-connected network, no matter how large.</p>
</blockquote>
<p>Using a <em>proof by induction</em>, it is actually possible to prove that after any number of rounds of training the two hidden units are still computing <em>identical functions</em>. This is often called the <strong>symmetry breaking problem</strong>.</p>
<p>The solution to this problem, is to initialize parameters <em>randomly</em>. Heres an example on how to do that with numpy:</p>
<ul>
<li>\(W^{[1]}\) = <code>np.random.rand(2,2) * 0.01</code></li>
<li>\(W^{[2]}\) = <code>np.random.rand(1,2) * 0.01</code></li>
<li>...</li>
</ul>
<blockquote>
<p>This will generate small, gaussian random values.</p>
</blockquote>
<ul>
<li>\(b^{[1]}\) = <code>np.zeros((2,1))</code></li>
<li>\(b^{[2]}\) = <code>0</code></li>
<li>...</li>
</ul>
<blockquote>
<p>In next weeks material, we will talk about how and when you might choose a different factor than \(0.01\) for initialization.</p>
</blockquote>
<p>It turns out the \(b\) does not have this symmetry breaking problem, because as long as the hidden units are computing different functions, the network will converge on different values of \(b\), and so it is fine to initialize it to zeros.</p>
<p><strong>Why do we initialize to small values?</strong></p>
<p>For a <em>sigmoid-like</em> activation function, large parameter weights (positive or negative) will make it more likely that \(z\) is very large (positive or negative) and thus \(dz\) will approach \(0\), <em>slowing down learning dramatically</em>.</p>
<blockquote>
<p>Note this is a less of an issue when using ReLu's, however many classification problems use sigmoid activations in their output layer.</p>
</blockquote>
<h2 id="week-4-deep-neural-networks">Week 4: Deep Neural Networks</h2>
<h3 id="what-is-a-deep-neural-network">What is a deep neural network?</h3>
<p>A deep neural network is simply a network with more than 1 hidden layer. Compared to logistic regression or a simple neural network with one hidden layer (which are considered <strong>shallow</strong> models) we say that a neural network with many hidden layers is a <strong>deep</strong> model, hence the terms <em>deep learning / deep neural networks</em>.</p>
<blockquote>
<p>Shallow Vs. deep is a matter of degree, the more hidden layers, the deeper the model.</p>
</blockquote>
<p><img alt="" src="https://s19.postimg.org/ku99a0jmb/shallow_vs_deep.png" /></p>
<p>Over the years, the machine learning and AI community has realized that deep networks are excellent function approximators, and are able to learn incredibly complex functions to map inputs to outputs.</p>
<blockquote>
<p>Note that is difficult to know in advance how deep a neural network needs to be to learn an effective mapping.</p>
</blockquote>
<h4 id="notation">Notation</h4>
<p>Lets go over the notation we will need using an example network</p>
<p><img alt="" src="https://s19.postimg.org/b9pmn6rqb/simple_deep_nn.png" /></p>
<ul>
<li>We will use \(L\) to denote the number of layers in the network (in this network \(L=4\))</li>
<li>\(n^{[l]}\) denotes the number of units in layers \(l\) (for example, in this network \(n^{[1]} = 5\))</li>
<li>\(a^{[l]}\) denotes the activations in layer \(l\)</li>
<li>\(W^{[l]}\), \(b^{[l]}\) denotes the weights and biases for \(z^{[l]}\)</li>
<li>\(x = a^{[0]}\) and \(\hat y = a^{[L]}\)</li>
</ul>
<h3 id="forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</h3>
<p>Forward propogation in a deep network just extends what we have already seen for forward propogation in a neural network by some number of layers. More specifically, for each layer \(l\) we perform the computations:</p>
<p>\[ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \]
\[ A^{[l]} = g^{[l]}(Z^{[l]}) \]</p>
<blockquote>
<p>Note that the above implementation is vectorized across all training examples. Matrices \(A^{[l]}\) and \(Z^{[l]}\) stacked column vectors pertaining to a single input example for layer \(l\).</p>
</blockquote>
<p>Finally, our predictions (the results of our output layer) are:</p>
<p>\[\hat Y = g(Z^{[L]}) = A^{[L]}\]</p>
<blockquote>
<p>Note that this solution is not completely vectorized, we still need an explicit for loop over our layers \(l = 0, 1, ..., L\)</p>
</blockquote>
<h4 id="getting-your-matrix-dimensions-right">Getting your matrix dimensions right</h4>
<p>When implementing a neural network, it is extremely important that we ensure our matrix dimensions "line up". A simple debugging tool for neural networks then, is pen and paper!</p>
<p>For a \(l\)-layered neural network, our dimensions are as follows:</p>
<ul>
<li>\(W^{[l]}: (n^{[l]}, n^{[l-1]})\)</li>
<li>\(b^{[l]}: (n^{[l]}, 1)\)</li>
<li>\(Z^{[l]}, A^{[l]}: (n^{[l]}, m)\)</li>
<li>\(A^{[0]} = X: (n^{[0]}, m)\)</li>
</ul>
<p>Where \(n^{[l]}\) is the number of units in layer \(l\).</p>
<blockquote>
<p>See <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations">this</a> video for a derivation of these dimensions.</p>
</blockquote>
<p>When implementing backpropagation, the dimensions are the same, i.e., the dimensions of \(W\), \(b\), \(A\) and \(Z\) are the same as \(dW\), \(db\), ...</p>
<h3 id="why-deep-representations">Why deep representations?</h3>
<p>Lets train to gain some intuition behind the success of deep representation for certain problem domains.</p>
<h4 id="what-is-a-deep-network-computing">What is a deep network computing?</h4>
<p>Lets take the example of image recognition. Perhaps you input a picture of a face, then you can think of the first layer of the neural network as an "edge detector".</p>
<p>The next layer can use the outputs from the previous layer, which can roughly be thought of as detected edges, and "group" them in order to detect parts of faces. Each neuron may become tuned to detect different parts of faces.</p>
<p>Finally, the output layer uses the output of the previous layer, detected features of a face, and compose them together to recognize a whole face.</p>
<p><a href="https://postimg.cc/image/slpz8zvlr/"><img alt="deep_representations.png" src="https://s19.postimg.cc/57hzx2doj/deep_representations.png" /></a></p>
<blockquote>
<p>The main intuition is that earlier layers detect "simpler" structures, and pass this information onto the next layer which can use it to detect increasingly complex structures.</p>
</blockquote>
<p>These general idea applies to other examples than just computer vision tasks (e.g., audio). Moreover, there is an analogy between deep representations in neural networks and how the brain works, however it can be dangerous to push these analogies too far.</p>
<h4 id="circuit-theory-and-deep-learning">Circuit theory and deep learning</h4>
<p>Circuit theory also provides us with a possible explanation as to why deep networks work so well for some tasks. Informally, there are function you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units to compute.</p>
<blockquote>
<p>Check out <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations">this</a> video starting at 5:36 for a deeper explanation of this.</p>
</blockquote>
<h3 id="building-blocks-of-deep-neural-networks">Building blocks of deep neural networks</h3>
<p>Lets take a more holistic approach and talk about all the building blocks of deep neural networks. Here is a deep neural network with a few hidden layers</p>
<p><a href="https://postimg.cc/image/fgkkwtgun/"><img alt="simple_deep_nn_2.png" src="https://s19.postimg.cc/wtuvboc5v/simple_deep_nn_2.png" /></a></p>
<p>Lets pick one layer, \(l\) and look at the computations involved.</p>
<p>For this layer \(l\), we have parameters \(W^{[l]}\) and \(b^{[l]}\). Our two major computation steps through this layer are:</p>
<p><strong>Forward Propagation</strong></p>
<ul>
<li>Input: \(a^{[l-1]}\)</li>
<li>Output: \(a^{[l]}\)</li>
<li>Linear function: \(z^{[l]} = W^{[l]}a^{[l-1] + b^{[l]}}\)</li>
<li>Activation function: \(a^{[l]} = g^{[l]}(z^{[l]})\)</li>
</ul>
<p>Because \(z^{[l]}, W^{[l]} and b^{[l]}\) are used in then backpropagation steps, it helps to cache theses values during forward propagation.</p>
<p><strong>Backwards Propagation</strong></p>
<ul>
<li>Input: \(da^{[l]}, cache(z^{[l]})\)</li>
<li>Output: \(da^{[l-1]}, dW^{[l]}, db^{[l]}\)</li>
</ul>
<p>The key insight, is that for every computation in forward propagation there is a corresponding computation in backwards propagation</p>
<p><a href="https://postimg.cc/image/ct3ctjjnz/"><img alt="forward_backward.png" src="https://s19.postimg.cc/nfx5yyrtf/forward_backward.png" /></a></p>
<p>So one iteration of training with a neural network involves feeding our inputs into the network (\(a^{[0]})\), performing forward propagation computing \(\hat y\), and using it to compute the loss and perform backpropagation through the network. This will produce all the derivatives of the parameters w.r.t the loss that we need to update the parameters for gradient descent.</p>
<h3 id="parameters-vs-hyperparameters">Parameters vs hyperparameters</h3>
<p>The <strong>parameters</strong> of your model are the <em>adaptive</em> values, \(W\) and \(b\) which are <em>learned</em> during training via gradient descent.</p>
<p>In contrast, <strong>hyperparameters</strong> are set before training and can be viewed as the "settings" of the learning algorithms. They have a direct effect on the eventual value of the parameters.</p>
<p>Examples include:</p>
<ul>
<li>number of iterations</li>
<li>learning rate</li>
<li>number of hidden layers \(L\)</li>
<li>number of hidden units \(n^{[1]}, n^{[2]}, ...\)</li>
<li>choice of activation function</li>
</ul>
<blockquote>
<p>the learning rate is sometimes called a parameter. We will follow the convetion of calling it a hyperparameter.</p>
</blockquote>
<p>It can be difficult to know the optimal hyperparameters in advance. Often, we start by simply trying out many values to see what works best, this allows us to build our intuition about the best hyperparameters to use. We will defer a deep discussion on how to choose hyperparameters to the next course.</p>
<h3 id="what-does-this-all-have-to-do-with-the-brain">What does this all have to do with the brain?</h3>
<p>At the risk of giving away the punch line, <em>not a whole lot</em>.</p>
<p>The most important mathematical components of a neural networks: <em>forward propagation</em> and <em>backwards propagation</em> are rather complex, and it has been difficult to convey the intuition behind these methods. As a result, the phrase, "it's like the brain" has become an easy, but dramatically oversimplified explanation. It also helps that this explanation has caught the publics imagination.  </p>
<p>There is a loose analogy to be drawn from a biological neuron and the neurons in our artificial neural networks. Both take inputs (derived from other neurons) process the information and propagate a signal forward.</p>
<p><img alt="" src="" /></p>
<p>However, even today neuroscientists don't fully understand what a neuron is doing when it receives and propagates a signal. Indeed, we have no idea on whether the biological brain is performing some algorithmic processes similar to those performed by an ANN.</p>
<p>Deep learning is an excellent method for complex function approximation, i.e., learning mappings from inputs \(x\) to outputs \(y\). However we should be very wary about pushing the, "its like a brain!" analogy too far.</p>
<h1 id="course-3-structuring-machine-learning-projects">Course 3: Structuring Machine Learning Projects</h1>
<h4 id="toc_1">TOC</h4>
<ol>
<li><a href="#ml-strategy-1">Week 1: ML Strategy 1</a></li>
<li><a href="#ml-strategy-2">Week 1: ML Strategy 2</a></li>
</ol>
<h2 id="week-1-ml-strategy-1">Week 1: ML Strategy (1)</h2>
<p>What is <em>machine learning strategy?</em> Lets start with a motivating example.</p>
<h3 id="introduction-to-ml-strategy">Introduction to ML strategy</h3>
<h4 id="why-ml-strategy">Why ML strategy</h4>
<p>Lets say you are working on a <strong>cat classifier</strong>. You have achieved 90% accuracy, but would like to improve performance even further. Your ideas for achieveing this are:</p>
<ul>
<li>collect more data</li>
<li>collect more diverse training set</li>
<li>train the algorithm longer with gradient descent</li>
<li>try adam (or other optimizers) instead of gradient descent</li>
<li>try dropout, add L2 regularization, change network architecture, ...</li>
</ul>
<p>This list is long, and so it becomes incredibly important to be able to identify ideas that are worth our time, and which ones we can likely discard.</p>
<p>This course will attempt to introduce a framework for making these decisions. In particular, we will focus on the organization of <em>deep learning-based projects</em>.</p>
<h4 id="orthogonalization">Orthogonalization</h4>
<p>One of the challenges with building deep learning systems is the number of things we can tune to improve performance (<em>many hyperparameters notwithstanding</em>).</p>
<p>Take the example of an old TV. They included many nobs for tuning the display position (x-axis position, y-axis position, rotation, etc...).</p>
<p><strong>Orthogonalization</strong> in this example refers to the TV designers decision to ensure each nob had one effect on the display and that these effects were <em>relative</em> to one another. If these nobs did more than one action and each actions magnitude was not relative to the other, it would become nearly impossible to tune the TV.</p>
<p>Take another example, driving a <strong>car</strong>. Imagine if there was multiple joysticks. One joystick modified \(0.3\) X steering angle \(- 0.8\) speed, and another \(2\) X steering angle \(+ 0.9\) speed. In theory, by tuning these two nobs we could drive the car, but this would be <em>much more difficult then separating the inputs into distinct input mechanisms</em>.</p>
<p><strong>Orthogonal</strong> refers to the idea that the <em>inputs</em> are aligned to the dimensions we want to control.</p>
<p><a href="https://postimg.cc/image/t547roobz/"><img alt="speed_v_angle_orth.png" src="https://s19.postimg.cc/51dg3e5v7/speed_v_angle_orth.png" /></a></p>
<p><em>How does this related to machine learning?</em></p>
<h5 id="chain-of-assumption-in-examples">Chain of assumption in examples</h5>
<p>For a machine learning system to perform "well", we usually aim to make four things happen:</p>
<ol>
<li>Fit training set well on cost function (for some applications, this means comparing favorably to human-level performance).</li>
<li>Fit dev set well on cost function</li>
<li>Fit test set well on cost function</li>
<li>Performs well in real world.</li>
</ol>
<p>If we relate back to the TV example, we wanted <em>one knob</em> to change each attribute of the display. <em>In the same way, we can modify knobs for each of our four steps above</em>:</p>
<ol>
<li>Train a bigger network, change the optimization algorithm, ...</li>
<li>Regularization, bigger training set, ...</li>
<li>Bigger dev set, ...</li>
<li>Change the dev set or the cost function</li>
</ol>
<blockquote>
<p>Andrew said when he trains neural networks, he tends <strong>not</strong> to use <strong>early stopping</strong>. The reason being is that this is not a very <strong>orthogonal</strong> "knob"; it simultaneously effects how well we fit the training set and the dev set.</p>
</blockquote>
<p>The whole idea here is that if we keep our "knobs" <strong>orthogonal</strong>, we can more easily come up with solutions to specific problems with our deep neural networks (i.e., if we are getting poor performance on the training set, we may opt to train a bigger [higher variance] network).</p>
<h3 id="setting-up-your-goal">Setting up your goal</h3>
<h4 id="single-number-evaluation-metric">Single number evaluation metric</h4>
<p>When tuning neural networks (modifying hyper-parameters, trying different architectures, etc.) you will find that having a <em>single <strong>evaluation metric</strong></em> will allow you to easily and quickly judge if a certain change improved performance.</p>
<blockquote>
<p>Andrew recommends deciding on a single, real-valued evaluation metric when starting out on your deep learning project.</p>
</blockquote>
<p>Lets look at an example.</p>
<p>As we discussed previously, <strong>applied machine learning</strong> is a very empirical process.</p>
<p><a href="https://postimg.cc/image/6t6eybcdb/"><img alt="using_a_single_number.png" src="https://s19.postimg.cc/3mbveorxf/using_a_single_number.png" /></a></p>
<p>Lets say that we start with classifier A, and end up with classifier B after some change to the model. We could look at <strong>precision</strong> and <strong>recall</strong> as a means of improvements. What we really want is to improve <em>both</em> precision and recall. The problem is that it can become difficult to choose the "best" classifier if we are monitoring two different performance metrics, especially when we are making many modifications to our network.</p>
<p>This is when it becomes important to chose a single performance metric. In this case specifically, we can chose the <strong>F1-score</strong>, the harmonic mean of the precision and recall (less formally, think of this as an average).</p>
<p><a href="https://postimg.cc/image/uwx6mln4f/"><img alt="chosing_f1_score.png" src="https://s19.postimg.cc/ovzhpj0ib/chosing_f1_score.png" /></a></p>
<p>We can see very quickly that classifier A has a better F1-score, and therefore we chose classifier A over classifier B.</p>
<h4 id="satisficing-and-optimizing-metric">Satisficing and Optimizing metric</h4>
<p>It is not always easy to combine all the metrics we care about into a single real-numbered value. Lets introduce <strong>satisficing</strong> and <strong>optimizing</strong> metrics as a solution to this problem.</p>
<p>Lets say we are building a classifier, and we care about both our <strong>accuracy</strong> (measured as F1-score, traditional accuracy or some other metric) <em>and</em> the <strong>running time</strong> to classify a new example.</p>
<p><a href="https://postimg.cc/image/aoizsfn67/"><img alt="two_metrics_optimize.png" src="https://s19.postimg.cc/px8x67gur/two_metrics_optimize.png" /></a></p>
<p>One thing we can do, is to combine accuracy and run-time into a <strong>single-metric</strong>, possibly by taking a weighted linear sum of the two metrics.</p>
<blockquote>
<p>As it turns out, this tends to produce a rather artificial solution (no pun intended).</p>
</blockquote>
<p>Another way, is to attempt to <em>maximize accuracy</em> while subject to the restraint that \(\text{running time} \le 100\)ms. In this case, we say that <em>accuracy</em> is an <strong>optimizing</strong> metric (because we want to maximize or minimize it) and <em>running time</em> is a <strong>satisficing</strong> metric (because it just needs to meet a certain constraint, i.e., be "good enough").</p>
<p>More generally, if we have \(m\) metrics that we care about, it is reasonable to choose <em>one</em> to be our <strong>optimizing metric</strong>, and \(m-1\) to be <strong>satisficing metrics</strong>.</p>
<h5 id="example-wake-words">Example: Wake words</h5>
<p>We can take a concrete example to illustrate this: <strong>wake words</strong> for <strong>intelligent voice assistants</strong>. We might chose the accuracy of the model (i.e., what percent of the time does it "wake" when a wake word is said) to be out <strong>optimizing metric</strong> s.t. we have \(\le 1\) false-positives per 24 hours of operation (our <strong>satisficing metric</strong>).</p>
<h5 id="summary">Summary</h5>
<p>To summarize, if there are multiple things you care about, we can set one as the <strong>optimizing metric</strong> that you want to do as well as possible on and one or more as <strong>satisficing metrics</strong> were you'll be satisfied. This idea goes hand-in-hand with the idea of having a single real-valued performance metric whereby we can <em>quickly</em> and <em>easily</em> chose the best model given a selection of models.</p>
<h3 id="traindevtest-distributions">Train/dev/test distributions</h3>
<p>The way you set up your train, dev (sometimes called valid) and test sets can have a large impact on your development times and even model performance.</p>
<p>In this video, we are going to focus on the <strong>dev</strong> (sometimes called the <strong>valid</strong> or <strong>hold out</strong> set) and the <strong>test set</strong>. The general workflow in machine learning is to train on the <strong>train</strong> set and test out model performance (e.g., different hyper-parameters or model architectures) on the <strong>dev</strong> set.</p>
<p>Lets look at an example. Say we had data from multiple regions:</p>
<ul>
<li>US</li>
<li>UK</li>
<li>Other European countries</li>
<li>South America</li>
<li>India</li>
<li>China</li>
<li>Other Asian countries</li>
<li>Australia</li>
</ul>
<p>If we were to build our dev set by choosing data from the first four regions and our test set from the last four regions, our data would likely be <strong>skewed</strong> and our model would likely perform poorly (at least on the <strong>test</strong> set). <em>Why?</em></p>
<p>Imagine the <strong>dev</strong> set as a target, and our job as machine learning engineers is to hit a bullseye. <em>A dev set that is not representative of the overall general distribution is analogous to moving the bullseye away from its original location moments after we fire our bow</em>. An ML team could spend months optimizing the model on a dev set, only to achieve very poor performance on a test set!</p>
<p>So for our data above, a much better idea would be to sample data randomly from all regions to build our <strong>dev</strong> and <strong>test</strong> set.</p>
<h4 id="guidelines">Guidelines</h4>
<p>Choose a <strong>dev</strong> set and <strong>test</strong> set (from the same distribution) to reflect data you expect to <em>get in the future</em> and <em>consider important to do well on</em>.</p>
<h3 id="size-of-the-dev-and-test-sets">Size of the dev and test sets</h3>
<p>In the last lecture we saw that the dev and test sets should come from the same distributions. <em>But how large should they be?</em></p>
<h4 id="size-of-the-devtest-sets">Size of the dev/test sets</h4>
<p>The rule of thumb in machine learning is typically 60% <strong>training</strong>, 20% <strong>dev</strong>, and 20% <strong>test</strong> (or 70/30 <strong>train</strong>/<strong>test</strong>). In earlier eras of machine learning, this was pretty reasonable. In the modern machine learning era, we are used to working with <em>much</em> larger data set sizes.</p>
<p>For example, imagine we have \(1,000,000\) examples. It might be totally reasonable for us to use 98% as our test set, 1% for dev and 1% for <strong>test</strong>.</p>
<blockquote>
<p>Note that 1% of \(10^6\) is \(10^4\)!</p>
</blockquote>
<h4 id="guidelines_1">Guidelines</h4>
<p>Set your <strong>test</strong> set to be big enough to give high confidence in the overall performance of your system.</p>
<h3 id="when-to-change-devtest-sets-and-metrics">When to change dev/test sets and metrics</h3>
<p>Sometimes during the course of a machine learning project, you will realize that you want to change your evaluation metric (i.e., move the "goal posts"). Lets illustrate this with an example:</p>
<h4 id="example-1">Example 1</h4>
<p>Imagine we have two models for image classification, and we are using classification performance as our evaluation metric:</p>
<ul>
<li>Algorithm A has a <strong>3%</strong> error, but sometimes shows users pornographic images.</li>
<li>Algorithm B has a <strong>5%</strong> error.</li>
</ul>
<p>Cleary, algorithm A performs better by our original evaluation metric (classification performance), but showing users pornographic images is <em>unacceptable</em>.</p>
<p>\[Error = \frac{1}{m_{dev}}\sum^{m_{dev}}<em>{i=1} \ell { y</em>{pred}^{(i)} \ne y^{(i)} }\]</p>
<blockquote>
<p>Our error treats all incorrect predictions the same, pornographic or otherwise.</p>
</blockquote>
<p>We can think of it like this: our evaluation metric <em>prefers</em> algorithm A, but <em>we</em> (and our users) prefer algorithm B. When our evaluation metric is no longer ranking the algorithms in the order we would like, it is a sign that we may want to change our evaluation metric. In our specific example, we could solve this by weighting misclassifications</p>
<p>\[Error = \frac{1}{w^{(i)}}\sum^{m_{dev}}<em>{i=1} w^{(i)}\ell { y</em>{pred}^{(i)} \ne y^{(i)} }\]</p>
<p>where \(w^{(i)}\) is 1 if \(x^{(i)}\) is non-porn and 10 (or even 100 or larger) if \(x^{(i)}\) is porn.</p>
<p>This is actually an example of <strong>orthogonalization</strong>. We,</p>
<ol>
<li>Define a metric to evaluate our model ("placing the target")</li>
<li>(In a completely separate step) Worry about how to do well on this metric.</li>
</ol>
<h4 id="example-2">Example 2</h4>
<p>Take the same example as above, but with a new twist. Say we train our classifier on a data set of high quality images. Then, when we deploy our model we notice it performs poorly. We narrow the problem down to the low quality images users are "feeding" to the model. What do we do?</p>
<p>In general: <em>if doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set</em>.</p>
<h3 id="comparing-to-human-level-performance">Comparing to human-level performance</h3>
<p>In the last few years, comparing machine learning systems to human level performance have become common place. The reasons for this include:</p>
<ol>
<li>Deep learning based approaches are making extraordinary gains in performance, so our baseline needs to be more stringent.</li>
<li>Many of the tasks deep learning is performing well at were thought to be very difficult for machines (e.g. NLP, computer vision). Comparing performance on these tasks to a human baseline is natural.</li>
</ol>
<p>It is also instructive to look at the performance of machine learning over time (note this is an obvious abstraction)</p>
<p><a href="https://postimg.cc/image/wm9ztnwof/"><img alt="ai_progress_over_time.png" src="https://s19.postimg.cc/8ij85de7n/ai_progress_over_time.png" /></a></p>
<p>Roughly speaking, performance (e.g., in a research domain or for a certain task) progresses quickly until we reach human-level performance, and tails off quickly. <em>Why?</em> mainly because human level performance is typically very close to the <a href="http://www.wikiwand.com/en/Bayes_error_rate"><strong>Bayes optimal error</strong></a>. Bayes optimal error is the best possible error; there is no way for any function mapping from \(x \rightarrow y\) to do any better. A second reason is that so long as ML performs worse than humans for a given task, we can:</p>
<ul>
<li>get labeled data from humans</li>
<li>gain insight from manual error analysis (e.g., why did a person get this right?)</li>
<li>better analysis of bias/variance</li>
</ul>
<h3 id="avoidable-bias">Avoidable bias</h3>
<p>Of course, we want our learning algorithm to perform well on the training set, but not <em>too well</em>. Knowing where human level performance is can help us decide how well we want to perform on the training set.</p>
<p>Let us again take the example of an image classifier. For this particular data set, assume:</p>
<ul>
<li>human-level performance is an error of 1%.</li>
<li>our classifier is currently achieving 8% classification error on the training set and</li>
<li>10% classification on the dev set.</li>
</ul>
<p><em>Clearly, it has plenty of room to improve</em>. Specifically, we would want to try to <em>increase</em> <strong>variance</strong> and <em>reduce</em> <strong>bias</strong>.</p>
<blockquote>
<p>For the purposes of computer vision, assume that human-level performance \(\approx\) Bayes error.</p>
</blockquote>
<p>Now, lets take the same example, but instead, we assume that human-level performance is an error of 7.5% (this example is very contrived, as humans are extremely good at image classification). In this case, we note that our classifier performances nearly as well as a human baseline. We would likely want to to <em>decrease</em> <strong>variance</strong> and <em>increase</em> <strong>bias</strong> (in order to improve performance on the <strong>dev</strong> set.)</p>
<p>So what did this example show us? When human-level performance (where we are using human-level performance as a proxy for Bayes error) is <em>very high</em> relative to our models performance on the train set, we likely want to focus on reducing  <em>"avoidable"</em> bias (or increasing variance) in order to improve performance on the training set (e.g., by using a bigger network.) When human-level performance is <em>comparable</em> to our models performance on the train set, we likely want to focus on increasing bias (or decreasing variance) in order to improve performance on the dev set (e.g., by using a regularization technique or gathering more training data.)</p>
<h3 id="understanding-human-level-performance">Understanding human-level performance</h3>
<p>The term <em>human-level performance</em> is used quite casually in many research articles. Lets attempt to define this term more precisely.</p>
<p>Recall from the last lecture that <strong>human-level performance</strong> can be used as a proxy for <strong>Bayes error</strong>. Lets revisit that idea with another example.</p>
<p>Suppose, for a medical image classification example,</p>
<ul>
<li>Typical human: 3% error</li>
<li>Typical doctor: 1% error</li>
<li>Experienced doctor: 0.7% error</li>
<li>Team of experienced doctors: 0.5% error</li>
</ul>
<p><em>What is "human-level" error?</em> Most likely, we would say <strong>0.5%</strong>, and thus Bayes error is \(\le 0.05%\).  However, in certain contexts we may only wish to perform as well as the typical doctor (i.e., 1% error) and we may deem this <em>"human-level error"</em>. The takeaway is that there is sometimes more than one way to determine human-level performance; which way is appropriate will depend on the context in which we expect our algorithm to be deployed. We also note that as the performance of our algorithm improves, we may decide to move the goal posts for human-level performance higher, e.g., in this example by choosing a team of experienced doctors as the baseline. This is useful for solving the problem introduced in the previous lecture: <em>should I focus on reducing avoidable bias? or should I focus on reducing variance between by training and dev errors.</em></p>
<h4 id="summary_1">Summary</h4>
<p>Lets summarize: if you are trying to understand bias and variance when you have a human-level performance baseline:</p>
<ul>
<li>Human-level error can be used as a proxy for Bayes' error</li>
<li>The difference between the training error and the human-level error can be thought of as the <strong>avoidable bias</strong>.</li>
<li>The difference between the training and dev errors can be thought of as <strong>variance</strong>.</li>
<li>Which type of error you should focus on reducing depends on how well your model perform compares to (an estimate of) human-level error.</li>
<li>As our model approaches human-level performance, it becomes harder to determine where we should focus our efforts.</li>
</ul>
<h3 id="surpassing-human-level-performance">Surpassing human-level performance</h3>
<p>Surpassing human-level performance is what many teams in machine learning / deep learning are inevitably trying to do. Lets take a look at a harder example to further develop our intuition for an approach to <em>matching</em> or <em>surpassing</em> human-level performance.</p>
<ul>
<li>team of humans: 0.5% error</li>
<li>one human: 1.0% error</li>
<li>training error: 0.3% error</li>
<li>dev error: 0.4% error</li>
</ul>
<p>Notice that training error &lt; team of humans error. Does this mean we have <em>overfit</em> the data by 0.2%? Or, does this means Bayes' error is actually lower than the team of humans error? We don't really know based on the information given, as to whether we should focus on <strong>bias</strong> or <strong>variance</strong>. This example is meant to illustrate that once we surpass human-level performance, it becomes much less clear how to improve performance further.</p>
<h4 id="problems-where-ml-significantly-surpasses-human-level-performance">Problems where ML significantly surpasses human-level performance</h4>
<p>Some example where ML <em>significantly surpasses human-level performance</em> include:</p>
<ul>
<li>Online advertising,</li>
<li>Product recommendations</li>
<li>Logistics (predicting transit time)</li>
<li>Load approvals</li>
</ul>
<p>Notice that many of these tasks are learned on <strong>structured data</strong> and do not involve <strong>natural perception tasks</strong>. This appeals to our intuition, as we know humans are <em>excellent</em> at natural perception tasks.</p>
<blockquote>
<p>We also note that these four tasks have immensely large datasets for learning.</p>
</blockquote>
<h3 id="improving-your-model-performance">Improving your model performance</h3>
<p>You have heard about orthogonalization. How to set up your dev and test sets, human level performance as a proxy for Bayes's error and how to estimate your avoidable bias and variance. Let's pull it all together into a set of guidelines for how to improve the performance of your learning algorithm.</p>
<h4 id="the-two-fundamental-assumptions-of-supervised-learning">The two fundamental assumptions of supervised learning</h4>
<ol>
<li>You can fit the training set (pretty) well, i.e., we can achieve <em>low avoidable bias</em>.</li>
<li>The training set performance generalizes pretty well to the dev/test set, i.e., variance is <em>not too bad</em>.</li>
</ol>
<p>In the spirit of orthogonalization, there are a certain set of (separate) knobs we can use to improve bias and variance. Often, the difference between the training error and Bayes error (or a human-level proxy) is often illuminating in terms of where large improvement remain to be made.</p>
<p><em>For reducing bias</em></p>
<ul>
<li>Train a bigger model</li>
<li>Train longer/better optimization algorithms</li>
<li>Change/tweak NN architecture/hyperparameter search.</li>
</ul>
<p><em>For reducing variance</em></p>
<ul>
<li>Collect more data</li>
<li>Regularization (L2, dropout, data augmentation)</li>
<li>Change/tweak NN architecture/hyperparameter search.</li>
</ul>
<h2 id="week-2-ml-strategy-2">Week 2: ML Strategy (2)</h2>
<h3 id="error-analysis">Error Analysis</h3>
<p>Manually examining mistakes that your algorithm is making can give you insights into what to do next (<em>especially if your learning algorithm is not yet at the performance of a human</em>). This process is called <strong>error analysis</strong>. Let's start with an example.</p>
<h4 id="carrying-out-error-analysis">Carrying out error analysis</h4>
<p>Take for example our <strong>cat image classifier</strong>, and say we obtain 10% error on our <strong>test set</strong>, much worse than we were hoping to do. Assume further that a colleague notices some of the misclassified examples are actually pictures of dogs. The question becomes, <em>should you try to make your cat classifier do better on dogs?</em></p>
<p>This is where <strong>error analysis</strong> is particularly useful. In this example, we might:</p>
<ul>
<li>collect ~100 mislabeled dev set examples</li>
<li>count up how any many dogs</li>
</ul>
<p>Lets say we find that 5/100 (5%) mislabeled dev set example are dogs. Thus, the best we could hope to do (if we were to <em>completely</em> solve the dog problem) is decrease our error from 10% to 9.5% (a 5% relative drop in error.) We conclude that <em>this is likely not the best use of our time</em>. Sometimes, this is called the <strong>ceiling</strong>, i.e., the <em>maximum</em> amount of improvement we can expect from <em>some change</em> to our algorithm/dataset.</p>
<p>Suppose instead we find 50/500 (50%) mislabeled dev set examples are dogs. Thus, if we solve the dog problem, we could decrease our error from 10% to 5% (a 50% relative drop in error.) We conclude that <em>solving the dog problem is likely a good use of our time</em>.</p>
<blockquote>
<p>Notice the disproportionate 'payoff' here. It may take &lt; 10 min to manually examine 100 examples from our dev set, but the exercise offers <em>major</em> clues as to where to focus our efforts.</p>
</blockquote>
<h5 id="evaluate-multiple-ideas-in-parallel">Evaluate multiple ideas in parallel</h5>
<p>Lets, continue with our cat detection example. Sometimes we might want to evaluate <strong>multiple</strong> ideas in <strong>parallel</strong>. For example, say we have the following ideas:</p>
<ul>
<li>fix pictures of dogs being recognized as cats</li>
<li>fix great cats (lions, panthers, etc..) being misrecognized</li>
<li>improve performance on blurry images</li>
</ul>
<p>What can do is create a table, where the <em>rows</em> represent the images we plan on evaluating manually, and the <em>columns</em> represent the categorizes we think the algorithm may be misrecognizing. It is also helpful to add comments describing the the misclassified example.</p>
<p><img alt="" src="https://s19.postimg.org/thwsxyhrn/Screen_Shot_2018-02-24_at_9.39.51_AM.png" /></p>
<p>As you are part-way through this process, you may also notice another common category of mistake, which you can add to this manual evaluation and repeat.</p>
<p><em>The conclusion of this process is estimates for:</em></p>
<ul>
<li>which errors we should direct our attention to solving</li>
<li>how much we should expect performance to improve if reduce the number of errors in each category</li>
</ul>
<h5 id="summary_2">Summary</h5>
<p>To summarize: when carrying out error analysis, you should find a set of <em>mislabeled</em> examples and look at these examples for <em>false positives</em> and <em>false negatives</em>. Counting up the number of errors that fall into various different categories will often this will help you prioritize, or give you inspiration for new directions to go in for improving your algorithm.</p>
<p>Three numbers to keep your eye on</p>
<ol>
<li>Overall dev set error</li>
<li>Errors due to cause of interest / Overall dev set error</li>
<li>Error due to other causes / Overall dev set error</li>
</ol>
<p>If the errors due to other causes &gt;&gt; errors due to cause of interest, it will likely be more productive to ignore our cause of interest for the time being and seek another source of error we can try to minimize.</p>
<blockquote>
<p>In this case, <em>cause of interest</em> is just our idea for improving our leaning algorithm, e.g., <em>fix pictures of dogs being recognized as cats</em></p>
</blockquote>
<h3 id="cleaning-up-incorrectly-labeled-data">Cleaning up incorrectly labeled data</h3>
<p>In supervised learning, we (typically) have hand-labeled training data. What if we realize that some examples are <em>incorrectly labeled?</em> First, lets consider our training set.</p>
<blockquote>
<p>In an effort to be less ambiguous, we use <strong>mislabeled</strong> when we are referring to examples the ML algo labeled incorrectly and <strong>incorrectly</strong> labeled when we are referring to examples in the training data set with the wrong label.</p>
</blockquote>
<h4 id="training-set">Training set</h4>
<p>Deep learning algorithms are quite robust to <strong>random</strong> errors in the training set. If the errors are reasonably <strong>random</strong> and the dataset is big enough (i.e., the errors make up only a tiny proportion of all examples) performance of our algorithm is unlikely to be affected.</p>
<p><strong>Systematic errors</strong> are much more of a problem. Taking as example our cat classifier again, if labelers mistakingly label all white dogs as cats, this will dramatically impact performance of our classifier, which is likely to labels white dogs as cats with <em>high degree of confidence</em>.</p>
<h4 id="devtest-set">Dev/test set</h4>
<p>If you suspect that there are many <em>incorrectly</em> labeled examples in your dev or test set, you can add another column to your error analysis table where you track these incorrectly labeled examples. Depending on the total percentage of these examples, you can decide if it is worth the time to go through and correct all <em>incorrectly</em> labeled examples in your dev or test set.</p>
<p>There are some special considerations when correcting incorrect dev/test set examples, namely:</p>
<ul>
<li>apply the same process to your dev and test sets to make sure they continue to come from the same distribution</li>
<li>considering examining examples your algorithm got right as well as ones it got wrong</li>
<li>train and dev/test data may now come from different distributions --- this is not necessarily a problem</li>
</ul>
<h4 id="build-quickly-then-iterate">Build quickly, then iterate</h4>
<p>If you are working on a brand new ML system, it is recommended to <em>build quickly</em>, then <em>iterate</em>. For many problems, there are often tens or hundreds of directions we could reasonably choose to go in.</p>
<p>Building a system quickly breaks down to the following tasks:</p>
<ol>
<li>set up a dev/test set and metric</li>
<li>build the initial system quickly and deploy</li>
<li>use bias/variance analysis &amp; error analysis to prioritize next steps</li>
</ol>
<p>A lot of value in this approach lies in the fact that we can quickly build insight to our problem.</p>
<blockquote>
<p>Note that this advice applies less when we have significant expertise in a given area and/or there is a significant body of academic work for the same or a very similar task (i.e., face recognition).</p>
</blockquote>
<h3 id="mismatched-training-and-devtest-set">Mismatched training and dev/test set</h3>
<p>Deep learning algorithms are <em>extremely data hungry</em>. Because of this, some teams are tempted into shoving as much information into their training sets as possible. However, this poses a problem when the data sources do not come from the same distributions.</p>
<p>Lets illustrate this again with an example. Take our cat classifier. Say we have ~10,000 images from a <strong>mobile app</strong>, and these are the images (or <em>type</em> of images) we hope to do well on. Assume as well that we have ~200,000 images from <strong>webpages</strong>, which have a slightly different underlying distribution than the mobile app images (say, for example, that they are generally higher quality.) <em>How do we combine these data sets?</em></p>
<h4 id="option-1">Option 1</h4>
<p>We could take the all datasets, combine them, and shuffle them randomly into train/dev/test sets. However, this poses the obvious problem that <em>many of the examples in our dev set (~95% of them) will be from the webpage dataset</em>. We are effectively tuning our algorithm to a distribution that is <em>slightly different</em> than our target distribution --- data from the mobile app.</p>
<h4 id="option-2">Option 2</h4>
<p>The second, recommended option, is to comprise the dev/test sets of images <em>entirely from the target (i.e., mobile data) distribution</em>. The advantage, is that we are now "aiming the target" in the right place, i.e., the distribution we hope to perform well on. The disadvantage of course, is that the training set comes from a different distribution than our target (dev/test) sets. However, this method is still superior to <strong>option 1</strong>, and we will discuss laters further ways of dealing with this difference in distributions.</p>
<blockquote>
<p>Note, we can still include examples from the distribution we care about in our training set, assuming we have enough data from this distribution.</p>
</blockquote>
<h3 id="bias-and-variance-with-mismatched-data-distributions">Bias and Variance with mismatched data distributions</h3>
<p>Estimating the <strong>bias</strong> and <strong>variance</strong> of your learning algorithm can really help you prioritize what to work on next. The way you analyze bias and variance changes when your training set comes from a different distribution than your dev and test sets. Let's see how.</p>
<p>Let's keep using our cat classification example and let's say humans get near perfect performance on this. So, Bayes error, or Bayes optimal error, we know is nearly 0% on this problem. Assume further:</p>
<ul>
<li>training error: 1%</li>
<li>dev error: 10%</li>
</ul>
<p>If your <strong>dev</strong> data came from the <em>same distribution</em> as your <strong>training</strong> set, you would say that you have a large <strong>variance</strong> problem, i.e., your algorithm is not generalizing well from the training set to the dev set. But in the setting where your training data and your dev data comes from a <em>different distribution</em>, you can no longer safely draw this conclusion. If the training and dev data come from <em>different underlying distributions</em>, then by comparing the training set to the dev set we are actually observing two different changes at the same time:</p>
<ol>
<li>The algorithm <em>saw</em> the training data. It did not <em>see</em> the dev data</li>
<li>The data do not come from the same underlying distribution</li>
</ol>
<p>In order to tease out these which of these is conributing to the drop in perfromsnce from our train to dev set, it will be useful to define a new piece of data which we'll call the <strong>training-dev</strong> set: a new subset of data with the same distribution as the training set, but not used for training.</p>
<p>Heres what we mean, previously we had train/dev/test sets. What we are going to do instead is randomly shuffle the training set and carve out a part of this shuffled set to be the <strong>training-dev</strong>.</p>
<p><img alt="" src="https://s19.postimg.org/hytnawr6r/Screen_Shot_2018-02-25_at_12.10.35_PM.png" /></p>
<blockquote>
<p>Just as the dev/test sets have the same distribution, the train-dev set and train set have the same distribution.</p>
</blockquote>
<p>Now, say we have the following errors:</p>
<ul>
<li>training error: 1%</li>
<li>train-dev error: 9%</li>
<li>dev error: 10%</li>
</ul>
<p>We see that training error \(\lt \lt\) train-dev error \(\approx\) dev error. Because the train and train-dev sets come from the same underlying distribution, we can safely conclude that the large increase in error from the train set to the dev set is due to <em>variance</em> (i.e., our network is not generalizing well)</p>
<p>Lets look at a counter example. Say we have the following errors:</p>
<ul>
<li>training error: 1%</li>
<li>train-dev error: 1.5%</li>
<li>dev error: 10%</li>
</ul>
<p>This is much more likely to be a <em>data mismatch problem</em>. Specifically, the algorithm is performing extremely well on the train and train-dev sets, but poorly on the dev set, hinting that the train/train-dev sets likely come from different underlying distributions than the dev set.</p>
<p>Finally, one last example. Say we have the following errors:</p>
<ul>
<li>Bayes error: \(\approx\) 0%</li>
<li>training error: 10%</li>
<li>train-dev error: 11%</li>
<li>dev error: 20%</li>
</ul>
<p>Here, we likely have two problems. First, we notice an <em>avoidable bias</em> problem, suggested by the fact that our training error \(\gt \gt\) Bayes error. We also have a <em>data mismatch problem</em>, suggested by the fact that our training error \(\approx\) train-dev error by both are \(\lt \lt\) our dev error.</p>
<p>So let's take what we've done and write out the general principles. The <em>key quantities</em> your want to look at are: human-level error (or Bayes error), training set error, training-dev set error and the dev set error.</p>
<p>The differences between these errors give us a sense about the <strong>avoidable bias</strong>, the <strong>variance</strong>, and the <strong>data mismatch problem</strong>. Generally,</p>
<ul>
<li>training error \(\gt \gt\) Bayes error: avoidable bias problem</li>
<li>training error \(\lt \lt\) train-dev error: variance problem</li>
<li>training error \(\approx\) train-dev error \(\lt \lt\) dev error: data mismatch problem.</li>
</ul>
<h4 id="more-general-formation">More general formation</h4>
<p>We can organize these metrics into a table; where the columns are different datasets (if you have more than one) and the rows are the error for examples the algorithm <em>was</em> trained on and examples the algorithm <em>was not</em> trained on.</p>
<p><img alt="" src="https://s19.postimg.org/bfl8ak6xv/Screen_Shot_2018-02-25_at_4.34.00_PM.png" /></p>
<h3 id="addressing-data-mismatch">Addressing data mismatch</h3>
<p>If your training set comes from a different distribution, than your dev and test set, and if error analysis shows you that you have a data mismatch problem, what can you do? Unfortunately, there are not (completely) systematic solutions to this, but let's look at some things you could try.</p>
<p><em>Some recommendations:</em></p>
<ul>
<li>carry out manual error analysis to try to understand different between training and dev/test sets.</li>
<li><em>for example, you may find that many of the examples in your dev set are noisy when compared to those in your training set.</em></li>
<li>make training data more similar; or collect more data similar to dev/test sets.</li>
<li><em>for example, you may simulate noise in the training set</em></li>
</ul>
<p>The second point leads us into the idea of <strong>artificial data synthesis</strong></p>
<h4 id="artificial-data-synthesis">Artificial data synthesis</h4>
<p>In some cases, we may be able to artificially synthesis data to make up for a lack of real data. For example, we can imagine synthesizing images of cars to supplement a dataset of car images for the task of car recognition in photos.</p>
<p><a href="https://postimg.cc/image/5rexjq01b/"><img alt="artificial_car_images.png" src="https://s19.postimg.cc/dk5lbp60j/artificial_car_images.png" /></a></p>
<p>While artificial data synthesis can be a powerful technique for increasing the size of our dataset (and thus the performance of our learning algorithm), we must be wary of overfitting to the synthesized data. Say for example, the set of "all cars" and "synthesized cars" looked as follows:</p>
<p><a href="https://postimg.cc/image/3mukint9r/"><img alt="artificial_data_venn.png" src="https://s19.postimg.cc/ojqsnbrar/artificial_data_venn.png" /></a></p>
<p>In this case, we run a real risk of our algorithm overfitting to the synthesized images.</p>
<h2 id="learning-from-multiple-tasks">Learning from multiple tasks</h2>
<h3 id="transfer-learning">Transfer learning</h3>
<p>One of the most powerful ideas in deep learning is that you can take knowledge the neural network has learned from <em>one task</em> and apply that knowledge to a <em>separate task</em>. So for example, maybe you could have the neural network learn to recognize objects like cats and then use parts of that knowledge to help you do a better job reading X-ray scans. This is called <strong>transfer learning</strong>. Let's take a look.</p>
<p>Lets say you have trained a neural network for <strong>image recognition</strong>. If you want to take this neural network and <em>transfer</em> it to a different task, say radiology diagnosis, one method would be to <em>delete</em> the last layer, and re-randomly initialize the weights feeding into the output layer.</p>
<p>To be concrete:</p>
<ul>
<li>during the first phase of training when you're training on an image recognition task, you train all of the usual parameters for the neural network, all the weights, all the layers</li>
<li>having trained that neural network, what you now do to implement transfer learning is swap in a new data set \(X,Y\), where now these are radiology images and diagnoses pairs.</li>
<li>finally, initialize the last layers' weights randomly and retrain the neural network on this new data set.</li>
</ul>
<p>We have a couple options on how we retrain the dataset.</p>
<ul>
<li>If the radiology dataset is <strong>small</strong>: we should likely <em>"freeze"</em> the transferred layers and only train the output layer.</li>
<li>If the radiology dataset is <strong>large</strong>: we should likely train all layers.</li>
</ul>
<blockquote>
<p>Sometimes, we call the process of training on the first dataset <strong>pre-training</strong>, and the process of training on the second dataset <strong>fine-tuning</strong>.</p>
</blockquote>
<p><img alt="" src="https://s19.postimg.org/ale9bpes3/Screen_Shot_2018-02-26_at_6.26.15_PM.png" /></p>
<p>The idea is that learning from a very large image data set allows us to transfer some fundamental knowledge for the task of computer vision (i.e., extracting features such as lines/edges, small objects, etc.)</p>
<blockquote>
<p>Note that transfer learning is <strong>not</strong> confined to computer vision examples, recent research has shown much success deploying transfer learning for NLP tasks.</p>
</blockquote>
<h4 id="when-does-transfer-learning-make-sense">When does transfer learning make sense?</h4>
<p>Transfer learning makes sense when you have a <em>lot of data for the problem you're transferring <strong>from</strong> and usually relatively less data for the problem you're transferring <strong>to</strong></em>.</p>
<p>So for our example, let's say you have a <em>million</em> examples for image recognition task. Thats a lot of data to learn low level features or to learn a lot of useful features in the earlier layers in neural network. But for the radiology task, assume we only a hundred examples. So a lot of knowledge you learn from image recognition can be transferred and can really help you get going with radiology recognition even if you don't have enough data to perform well for the radiology diagnosis task.</p>
<p>If you're trying to learn from some <strong>Task A</strong> and transfer some of the knowledge to some <strong>Task B</strong>, then transfer learning makes sense when:</p>
<ul>
<li>Task A and B have the same input X.</li>
<li>you have a lot more data for Task A than for Task B --- all this is under the assumption that what you really want to do well on is Task B.</li>
<li>transfer learning will tend to make more sense if you suspect that low level features from Task A could be helpful for learning Task B.</li>
</ul>
<h3 id="multi-task-learning">Multi-task learning</h3>
<p>Whereas in transfer learning, you have a sequential process where you learn from task A and then transfer that to task B --- in multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. The idea is that shared information from each of these tasks improves performance on <em>all</em> tasks. Let's look at an example.</p>
<h4 id="simplified-autonomous-driving-example">Simplified autonomous driving example</h4>
<p>Let's say you're building an autonomous vehicle. Then your self driving car would need to detect several different things such as <em>pedestrians</em>, <em>other cars</em>, <em>stop signs</em>, <em>traffic lights</em> etc.</p>
<p>Our input to the learning algorithm could be a single image, our our label for that example, \(y^{(i)}\) might be a four-dimensional column vector, where \(0\) at position \(j\) represents absence of that object from the image and \(1\) represents presence.</p>
<blockquote>
<p>E.g., a \(0\) at the first index of \(y^{(i)}\) might specify absence of a pedestrian in the image.</p>
</blockquote>
<p>Our neural network architecture would then involve a single input and output layer. The twist is that the output layer would have \(j\) number of nodes, one per object we want to recognize.</p>
<p><img alt="" src="https://s19.postimg.org/et91kny9v/Screen_Shot_2018-02-26_at_7.23.04_PM.png" /></p>
<p>To account for this, our cost function will need to sum over the individual loss functions for each of the objects we wish to recongize:</p>
<p>\[Cost = \frac{1}{m}\sum^m_{i=1}\sum^m_{j=1}\ell(\hat y_j^{(i)}, y_j^{(i)})\]</p>
<blockquote>
<p>Were \(\ell\) is our logisitc loss.</p>
</blockquote>
<p>Unlike traditional softmax regression, one image can have multiple labels. This, in essense, is <strong>multi-task</strong> learning, as we are preforming multiple tasks with the same neural network (sets of weights/biases).</p>
<h4 id="when-does-multi-task-learning-make-sense">When does multi-task learning make sense?</h4>
<p>Typically (but with some exceptions) when the following hold:</p>
<ul>
<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>
<li>Amount of data you have for each task is quite similar.</li>
<li>Can train a big enough neural network to do well on all the tasks</li>
</ul>
<blockquote>
<p>The last point is important. We typically need to "scale-up" the neural network in multi-task learning, as we will need a high variance model to be able to perform well on multiple tasks and typically more data --- as opposed to single tasks.</p>
</blockquote>
<h2 id="end-to-end-deep-learning">End-to-end deep learning</h2>
<p>One of the most exciting recent developments in deep learning has been the rise of <strong>end-to-end deep</strong> learning. So what is the end-to-end learning? Briefly, there have been some data processing systems, or learning systems that require <em>multiple stages of processing</em>. In contrast, end-to-end deep learning attempts to replace those multiple stages with a single neural network. Let's look at some examples.</p>
<h3 id="what-is-end-to-end-deep-learning">What is end-to-end deep learning?</h3>
<h4 id="speech-recognition-example">Speech recognition example</h4>
<p>At a high level, the task of speech recognition requires receiving as input some audio singles containing spoken words, and mapping that to a transcript containing those words.</p>
<p>Traditionally, speech recognition involved many stages of processing:</p>
<ol>
<li>First, you would extract "hand-designed" features from the audio clip</li>
<li>Feed these features into a ML algorithm which would extract phonemes</li>
<li>Concatenate these phonemes to form words and then transcripts</li>
</ol>
<p>In contrast to this step-by-step pipeline, <strong>end-to-end deep learning</strong> seeks to model all these tasks with a single network given a set of inputs.</p>
<p><img alt="" src="https://s19.postimg.org/cg5t1jlur/Screen_Shot_2018-02-27_at_8.53.09_PM.png" /></p>
<p>The more traditional, <strong>hand-crafted</strong> approach tends to <em>outperform</em> the <strong>end-to-end approach</strong> when <em>our dataset is small</em>, but this relationship flips as the dataset grows larger. Indeed, one of the biggest barriers to using end-to-end deep learning approaches is that large datasets which map our input to our final downstream task are <em>rare</em>.</p>
<blockquote>
<p>Think about this for a second and it makes perfect sense, its only recently in the era of deep learning that datasets have begun to map inputs to downstream outputs, skipping many of the intermediate levels of representation (images \(\Rightarrow\) labels, audio clips \(\Rightarrow\) transcripts.)</p>
</blockquote>
<p>One example where end-to-end deep learning currently works very well is <strong>machine translation</strong> (massive, parallel corpuses have made end-to-end solutions feasible.)</p>
<h3 id="summary_3">Summary</h3>
<p>When end-to-end deep learning works, it can work really well and can simplify the system, removing the need to build many hand-designed individual components. But it's also not panacea, <em>it doesn't always work</em>.</p>
<h3 id="whether-or-not-to-use-end-to-end-learning">Whether or not to use end-to-end learning</h3>
<p>Let's say in building a machine learning system you're trying to decide whether or not to use an end-to-end approach. Let's take a look at some of the pros and cons of end-to-end deep learning so that you can come away with some guidelines on whether or not an end-to-end approach seems promising for your application.</p>
<h4 id="pros-and-cons-of-end-to-end-deep-learning">Pros and cons of end-to-end deep learning</h4>
<p><strong>Pros</strong>:</p>
<ol>
<li><em>let the data speak</em>: if you have enough labeled data, your network (given that it is large enough) should be able to a mapping from \(x \rightarrow  y\), with out having to rely on a humans preconceived notions or forcing the model to use some representation of the relationship between inputs an outputs.</li>
<li><em>less hand-designing of components needed</em>: end-to-end deep learning seeks to model the entire task with a single learning algorithm, which typically involves little in the way of hand-designing components.</li>
</ol>
<p><strong>Cons</strong>:</p>
<ol>
<li><em>likely need a large amount of data for end-to-end learning to work well</em></li>
<li><em>excludes potentially useful hand-designed components</em>: if we have only a small training set, our learning algorithm likely does not have enough examples to learn representations that perform well. Although deep learning practitioners often speak despairingly about hand-crafted features or components, they allow us to inject priors into our model, which is particularly useful when we do not have a lot of labeled data.</li>
</ol>
<blockquote>
<p>Note: hand-designed components and features are a double-edged sword. Poorly designed components may actually harm performance of the model by forcing the model to obey incorrect assumptions about the data.</p>
</blockquote>
<h4 id="should-i-use-end-to-end-deep-learning">Should I use end-to-end deep learning?</h4>
<p>The key question we need to ask ourselves when considering on using end-to-end deep learning is:</p>
<p><em>Do you have sufficient data to learn a function of the complexity needed to map \(x\) to \(y\)?</em></p>
<p>Unfortunately, we do not have a formal definition of <strong>complexity</strong> --- we have to rely on our intuition.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
        
          <a href="sequence_models/week_1/" title="Week 1" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 1
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="./assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>